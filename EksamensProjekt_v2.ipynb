{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics\n",
    "\n",
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
