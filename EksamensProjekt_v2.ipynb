{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics\n",
    "\n",
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values from the data frame\n",
    "df_merged = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Location</th>\n",
       "      <th>Country Location</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Praskoveya</td>\n",
       "      <td>51</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>36</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>(55.755826, 37.6173)</td>\n",
       "      <td>(61.52401, 105.318756)</td>\n",
       "      <td>55.755826</td>\n",
       "      <td>37.6173</td>\n",
       "      <td>61.52401</td>\n",
       "      <td>105.318756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Pyatigorsk</td>\n",
       "      <td>54</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>(55.755826, 37.6173)</td>\n",
       "      <td>(61.52401, 105.318756)</td>\n",
       "      <td>55.755826</td>\n",
       "      <td>37.6173</td>\n",
       "      <td>61.52401</td>\n",
       "      <td>105.318756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Polevskoy</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>(55.755826, 37.6173)</td>\n",
       "      <td>(61.52401, 105.318756)</td>\n",
       "      <td>55.755826</td>\n",
       "      <td>37.6173</td>\n",
       "      <td>61.52401</td>\n",
       "      <td>105.318756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Dalnegorsk</td>\n",
       "      <td>29</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>29</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>(55.755826, 37.6173)</td>\n",
       "      <td>(61.52401, 105.318756)</td>\n",
       "      <td>55.755826</td>\n",
       "      <td>37.6173</td>\n",
       "      <td>61.52401</td>\n",
       "      <td>105.318756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Gukovo</td>\n",
       "      <td>38</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>38</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2511.0</td>\n",
       "      <td>(55.755826, 37.6173)</td>\n",
       "      <td>(61.52401, 105.318756)</td>\n",
       "      <td>55.755826</td>\n",
       "      <td>37.6173</td>\n",
       "      <td>61.52401</td>\n",
       "      <td>105.318756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country        City  AQI Value AQI Category  CO AQI Value CO AQI Category  \\\n",
       "0  Russia  Praskoveya         51     Moderate             1            Good   \n",
       "1  Russia  Pyatigorsk         54     Moderate             1            Good   \n",
       "2  Russia   Polevskoy         31         Good             1            Good   \n",
       "3  Russia  Dalnegorsk         29         Good             0            Good   \n",
       "4  Russia      Gukovo         38         Good             1            Good   \n",
       "\n",
       "   Ozone AQI Value Ozone AQI Category  NO2 AQI Value NO2 AQI Category  ...  \\\n",
       "0               36               Good              0             Good  ...   \n",
       "1               41               Good              1             Good  ...   \n",
       "2               31               Good              0             Good  ...   \n",
       "3               29               Good              0             Good  ...   \n",
       "4               38               Good              0             Good  ...   \n",
       "\n",
       "   GDP Year Average annual temperature (in Celsius)​ ​Average altitude (m)  \\\n",
       "0       NaN                                      6.7                 156.0   \n",
       "1       NaN                                      6.7                 156.0   \n",
       "2       NaN                                      6.7                 156.0   \n",
       "3       NaN                                      6.7                 156.0   \n",
       "4       NaN                                      6.7                 156.0   \n",
       "\n",
       "  ​Land area (in square km)         City Location        Country Location  \\\n",
       "0                    2511.0  (55.755826, 37.6173)  (61.52401, 105.318756)   \n",
       "1                    2511.0  (55.755826, 37.6173)  (61.52401, 105.318756)   \n",
       "2                    2511.0  (55.755826, 37.6173)  (61.52401, 105.318756)   \n",
       "3                    2511.0  (55.755826, 37.6173)  (61.52401, 105.318756)   \n",
       "4                    2511.0  (55.755826, 37.6173)  (61.52401, 105.318756)   \n",
       "\n",
       "   City Latitude  City Longitude  Country Latitude  Country Longitude  \n",
       "0      55.755826         37.6173          61.52401         105.318756  \n",
       "1      55.755826         37.6173          61.52401         105.318756  \n",
       "2      55.755826         37.6173          61.52401         105.318756  \n",
       "3      55.755826         37.6173          61.52401         105.318756  \n",
       "4      55.755826         37.6173          61.52401         105.318756  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['C40'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_merged\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC40\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['C40'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country               23037\n",
       "City                  23462\n",
       "AQI Value             23463\n",
       "AQI Category          23463\n",
       "CO AQI Value          23463\n",
       "CO AQI Category       23463\n",
       "Ozone AQI Value       23463\n",
       "Ozone AQI Category    23463\n",
       "NO2 AQI Value         23463\n",
       "NO2 AQI Category      23463\n",
       "PM2.5 AQI Value       23463\n",
       "PM2.5 AQI Category    23463\n",
       "Continent             23036\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\") # save df to a pickle file so it can be used for streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
