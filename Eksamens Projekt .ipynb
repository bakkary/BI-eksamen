{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8657e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fa15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa693206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5650ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23463, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae3337fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c63edc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54e8b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288622, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a3c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                              0\n",
       "City                                                 1\n",
       "AQI Value                                            0\n",
       "AQI Category                                         0\n",
       "CO AQI Value                                         0\n",
       "CO AQI Category                                      0\n",
       "Ozone AQI Value                                      0\n",
       "Ozone AQI Category                                   0\n",
       "NO2 AQI Value                                        0\n",
       "NO2 AQI Category                                     0\n",
       "PM2.5 AQI Value                                      0\n",
       "PM2.5 AQI Category                                   0\n",
       "Account number                                       0\n",
       "Organization                                         0\n",
       "Region                                               0\n",
       "C40                                             237710\n",
       "Access                                               0\n",
       "Reporting year                                       0\n",
       "Accounting year                                      0\n",
       "Boundary                                             0\n",
       "Protocol                                             0\n",
       "Protocol column                                 114244\n",
       "Gases included                                  149081\n",
       "Total emissions (metric tonnes CO2e)             20562\n",
       "Total Scope 1 Emissions (metric tonnes CO2e)     57609\n",
       "Total Scope 2 Emissions (metric tonnes CO2e)     61085\n",
       "Comment                                         191552\n",
       "Increase/Decrease from last year                 16018\n",
       "Reason for increase/decrease in emissions        63321\n",
       "Population                                           0\n",
       "Population year                                      0\n",
       "GDP                                              60666\n",
       "GDP Currency                                     58996\n",
       "GDP Year                                         59442\n",
       "GDP Source                                       58727\n",
       "Average annual temperature (in Celsius)​          5394\n",
       "​Average altitude (m)                            20526\n",
       "​Land area (in square km)                         1562\n",
       "City Location                                        0\n",
       "Country Location                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c8ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f4054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values from the data frame\n",
    "df_merged = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c3ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     0\n",
       "City                                        0\n",
       "AQI Value                                   0\n",
       "AQI Category                                0\n",
       "CO AQI Value                                0\n",
       "CO AQI Category                             0\n",
       "Ozone AQI Value                             0\n",
       "Ozone AQI Category                          0\n",
       "NO2 AQI Value                               0\n",
       "NO2 AQI Category                            0\n",
       "PM2.5 AQI Value                             0\n",
       "PM2.5 AQI Category                          0\n",
       "Region                                      0\n",
       "C40                                         0\n",
       "Reporting year                              0\n",
       "Total emissions (metric tonnes CO2e)        0\n",
       "Population                                  0\n",
       "GDP                                         0\n",
       "GDP Year                                    0\n",
       "Average annual temperature (in Celsius)​    0\n",
       "​Average altitude (m)                       0\n",
       "​Land area (in square km)                   0\n",
       "City Location                               0\n",
       "Country Location                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data frame for null values\n",
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2328ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195190\n",
       "City                                        195190\n",
       "AQI Value                                   195190\n",
       "AQI Category                                195190\n",
       "CO AQI Value                                195190\n",
       "CO AQI Category                             195190\n",
       "Ozone AQI Value                             195190\n",
       "Ozone AQI Category                          195190\n",
       "NO2 AQI Value                               195190\n",
       "NO2 AQI Category                            195190\n",
       "PM2.5 AQI Value                             195190\n",
       "PM2.5 AQI Category                          195190\n",
       "Region                                      195190\n",
       "C40                                         195190\n",
       "Reporting year                              195190\n",
       "Total emissions (metric tonnes CO2e)        195190\n",
       "Population                                  195190\n",
       "GDP                                         195190\n",
       "GDP Year                                    195190\n",
       "Average annual temperature (in Celsius)​    195190\n",
       "​Average altitude (m)                       195190\n",
       "​Land area (in square km)                   195190\n",
       "City Location                               195190\n",
       "Country Location                            195190\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf8382b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Location</th>\n",
       "      <th>Country Location</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>(-12.97304, -38.502304)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-12.97304</td>\n",
       "      <td>-38.502304</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.1</td>\n",
       "      <td>(-22.892857, -43.118381)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-22.892857</td>\n",
       "      <td>-43.118381</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>(-27.5949884, -48.5481743)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-27.5949884</td>\n",
       "      <td>-48.5481743</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>(-19.916681, -43.934493)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-19.916681</td>\n",
       "      <td>-43.934493</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>749.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>(-16.6868912, -49.2647943)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-16.6868912</td>\n",
       "      <td>-49.2647943</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country              City  AQI Value AQI Category  CO AQI Value  \\\n",
       "1241  Brazil  Presidente Dutra         41         Good             1   \n",
       "1242  Brazil  Presidente Dutra         41         Good             1   \n",
       "1243  Brazil  Presidente Dutra         41         Good             1   \n",
       "1244  Brazil  Presidente Dutra         41         Good             1   \n",
       "1246  Brazil  Presidente Dutra         41         Good             1   \n",
       "\n",
       "     CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "1241            Good                5               Good              1   \n",
       "1242            Good                5               Good              1   \n",
       "1243            Good                5               Good              1   \n",
       "1244            Good                5               Good              1   \n",
       "1246            Good                5               Good              1   \n",
       "\n",
       "     NO2 AQI Category  ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "1241             Good  ...    2012.0                                     26.0   \n",
       "1242             Good  ...    2013.0                                     23.0   \n",
       "1243             Good  ...    2013.0                                     20.0   \n",
       "1244             Good  ...    2014.0                                     21.0   \n",
       "1246             Good  ...    2010.0                                     23.2   \n",
       "\n",
       "     ​Average altitude (m) ​Land area (in square km)  \\\n",
       "1241                   8.0                     692.0   \n",
       "1242                   5.0                     133.1   \n",
       "1243                   3.0                     438.0   \n",
       "1244                 900.0                     331.0   \n",
       "1246                 749.0                     739.0   \n",
       "\n",
       "                   City Location         Country Location  City Latitude  \\\n",
       "1241     (-12.97304, -38.502304)  (-14.235004, -51.92528)      -12.97304   \n",
       "1242    (-22.892857, -43.118381)  (-14.235004, -51.92528)     -22.892857   \n",
       "1243  (-27.5949884, -48.5481743)  (-14.235004, -51.92528)    -27.5949884   \n",
       "1244    (-19.916681, -43.934493)  (-14.235004, -51.92528)     -19.916681   \n",
       "1246  (-16.6868912, -49.2647943)  (-14.235004, -51.92528)    -16.6868912   \n",
       "\n",
       "      City Longitude  Country Latitude  Country Longitude  \n",
       "1241      -38.502304        -14.235004          -51.92528  \n",
       "1242      -43.118381        -14.235004          -51.92528  \n",
       "1243     -48.5481743        -14.235004          -51.92528  \n",
       "1244      -43.934493        -14.235004          -51.92528  \n",
       "1246     -49.2647943        -14.235004          -51.92528  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be5b9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b08ec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c658a0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58e5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b97ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf32c49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81588</th>\n",
       "      <td>USA</td>\n",
       "      <td>Defiance</td>\n",
       "      <td>52</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>22</td>\n",
       "      <td>Good</td>\n",
       "      <td>7</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3692.0</td>\n",
       "      <td>47.6062</td>\n",
       "      <td>-122.3321</td>\n",
       "      <td>37.09024</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country      City  AQI Value AQI Category  CO AQI Value CO AQI Category  \\\n",
       "81588     USA  Defiance         52     Moderate             1            Good   \n",
       "\n",
       "       Ozone AQI Value Ozone AQI Category  NO2 AQI Value NO2 AQI Category  \\\n",
       "81588               22               Good              7             Good   \n",
       "\n",
       "       ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "81588  ...    2014.0                                     11.0   \n",
       "\n",
       "      ​Average altitude (m)  ​Land area (in square km)  City Latitude  \\\n",
       "81588                  15.0                     3692.0        47.6062   \n",
       "\n",
       "       City Longitude  Country Latitude  Country Longitude  C40_True  \\\n",
       "81588       -122.3321          37.09024         -95.712891         1   \n",
       "\n",
       "       C40_False  \n",
       "81588          0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba503efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195190 entries, 1241 to 288621\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Country                                   195190 non-null  object \n",
      " 1   City                                      195190 non-null  object \n",
      " 2   AQI Value                                 195190 non-null  int64  \n",
      " 3   AQI Category                              195190 non-null  object \n",
      " 4   CO AQI Value                              195190 non-null  int64  \n",
      " 5   CO AQI Category                           195190 non-null  object \n",
      " 6   Ozone AQI Value                           195190 non-null  int64  \n",
      " 7   Ozone AQI Category                        195190 non-null  object \n",
      " 8   NO2 AQI Value                             195190 non-null  int64  \n",
      " 9   NO2 AQI Category                          195190 non-null  object \n",
      " 10  PM2.5 AQI Value                           195190 non-null  int64  \n",
      " 11  PM2.5 AQI Category                        195190 non-null  object \n",
      " 12  Region                                    195190 non-null  object \n",
      " 13  Reporting year                            195190 non-null  int64  \n",
      " 14  Total emissions (metric tonnes CO2e)      195190 non-null  float64\n",
      " 15  Population                                195190 non-null  int64  \n",
      " 16  GDP                                       195190 non-null  float64\n",
      " 17  GDP Year                                  195190 non-null  float64\n",
      " 18  Average annual temperature (in Celsius)​  195190 non-null  float64\n",
      " 19  ​Average altitude (m)                     195190 non-null  float64\n",
      " 20  ​Land area (in square km)                 195190 non-null  float64\n",
      " 21  City Latitude                             195190 non-null  float64\n",
      " 22  City Longitude                            195190 non-null  float64\n",
      " 23  Country Latitude                          195190 non-null  float64\n",
      " 24  Country Longitude                         195190 non-null  float64\n",
      " 25  C40_True                                  195190 non-null  int32  \n",
      " 26  C40_False                                 195190 non-null  int32  \n",
      "dtypes: float64(10), int32(2), int64(7), object(8)\n",
      "memory usage: 40.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "761cee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's common_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a16c832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195190\n",
       "City                                        195190\n",
       "AQI Value                                   195190\n",
       "AQI Category                                195190\n",
       "CO AQI Value                                195190\n",
       "CO AQI Category                             195190\n",
       "Ozone AQI Value                             195190\n",
       "Ozone AQI Category                          195190\n",
       "NO2 AQI Value                               195190\n",
       "NO2 AQI Category                            195190\n",
       "PM2.5 AQI Value                             195190\n",
       "PM2.5 AQI Category                          195190\n",
       "Region                                      195190\n",
       "Reporting year                              195190\n",
       "Total emissions (metric tonnes CO2e)        195190\n",
       "Population                                  195190\n",
       "GDP                                         195190\n",
       "GDP Year                                    195190\n",
       "Average annual temperature (in Celsius)​    195190\n",
       "​Average altitude (m)                       195190\n",
       "​Land area (in square km)                   195190\n",
       "City Latitude                               195190\n",
       "City Longitude                              195190\n",
       "Country Latitude                            195190\n",
       "Country Longitude                           195190\n",
       "C40_True                                    195190\n",
       "C40_False                                   195190\n",
       "Continent                                   195189\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e50160f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "      <th>Continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22487</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Angatuba</td>\n",
       "      <td>18</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>5780.0</td>\n",
       "      <td>-15.794229</td>\n",
       "      <td>-47.882166</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.925280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Palmas</td>\n",
       "      <td>163</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>-25.431063</td>\n",
       "      <td>-49.264693</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.925280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192378</th>\n",
       "      <td>USA</td>\n",
       "      <td>Speedway</td>\n",
       "      <td>56</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>39</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>34.000700</td>\n",
       "      <td>-81.034800</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70523</th>\n",
       "      <td>USA</td>\n",
       "      <td>Brooklyn Center</td>\n",
       "      <td>51</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>35.199200</td>\n",
       "      <td>-111.631100</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97924</th>\n",
       "      <td>USA</td>\n",
       "      <td>Colchester</td>\n",
       "      <td>39</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>24</td>\n",
       "      <td>Good</td>\n",
       "      <td>9</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.195000</td>\n",
       "      <td>-106.837000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49690</th>\n",
       "      <td>USA</td>\n",
       "      <td>Annandale</td>\n",
       "      <td>50</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>50</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>39.103100</td>\n",
       "      <td>-84.512000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185831</th>\n",
       "      <td>USA</td>\n",
       "      <td>Topsham</td>\n",
       "      <td>46</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>21</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4</td>\n",
       "      <td>199.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>41.499300</td>\n",
       "      <td>-81.694400</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273960</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Carluke</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>15</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.481581</td>\n",
       "      <td>-3.179090</td>\n",
       "      <td>55.378051</td>\n",
       "      <td>-3.435973</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88928</th>\n",
       "      <td>USA</td>\n",
       "      <td>Gautier</td>\n",
       "      <td>51</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>42</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.724930</td>\n",
       "      <td>-122.156077</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218024</th>\n",
       "      <td>USA</td>\n",
       "      <td>Fernley</td>\n",
       "      <td>34</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>34</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>35.199200</td>\n",
       "      <td>-111.631100</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Country             City  AQI Value AQI Category  CO AQI Value  \\\n",
       "22487           Brazil         Angatuba         18         Good             1   \n",
       "16354           Brazil           Palmas        163    Unhealthy             6   \n",
       "192378             USA         Speedway         56     Moderate             1   \n",
       "70523              USA  Brooklyn Center         51     Moderate             1   \n",
       "97924              USA       Colchester         39         Good             1   \n",
       "49690              USA        Annandale         50         Good             1   \n",
       "185831             USA          Topsham         46         Good             1   \n",
       "273960  United Kingdom          Carluke         31         Good             1   \n",
       "88928              USA          Gautier         51     Moderate             1   \n",
       "218024             USA          Fernley         34         Good             1   \n",
       "\n",
       "       CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "22487             Good                8               Good              1   \n",
       "16354             Good                4               Good              3   \n",
       "192378            Good               39               Good              2   \n",
       "70523             Good               31               Good              6   \n",
       "97924             Good               24               Good              9   \n",
       "49690             Good               50               Good              1   \n",
       "185831            Good               21               Good              6   \n",
       "273960            Good               15               Good              8   \n",
       "88928             Good               42               Good              1   \n",
       "218024            Good               34               Good              3   \n",
       "\n",
       "       NO2 AQI Category  ...  Average annual temperature (in Celsius)​  \\\n",
       "22487              Good  ...                                      21.0   \n",
       "16354              Good  ...                                      18.0   \n",
       "192378             Good  ...                                      32.0   \n",
       "70523              Good  ...                                       6.5   \n",
       "97924              Good  ...                                       6.0   \n",
       "49690              Good  ...                                      13.0   \n",
       "185831             Good  ...                                      10.4   \n",
       "273960             Good  ...                                       9.7   \n",
       "88928              Good  ...                                      15.0   \n",
       "218024             Good  ...                                       6.5   \n",
       "\n",
       "       ​Average altitude (m) ​Land area (in square km)  City Latitude  \\\n",
       "22487                 1172.0                    5780.0     -15.794229   \n",
       "16354                  935.0                     434.0     -25.431063   \n",
       "192378                 292.0                     132.0      34.000700   \n",
       "70523                 2134.0                     165.0      35.199200   \n",
       "97924                 2405.0                      20.0      39.195000   \n",
       "49690                  168.0                     124.0      39.103100   \n",
       "185831                 199.0                     201.0      41.499300   \n",
       "273960                  65.0                     140.0      51.481581   \n",
       "88928                   15.0                      40.0      37.724930   \n",
       "218024                2134.0                     165.0      35.199200   \n",
       "\n",
       "        City Longitude  Country Latitude  Country Longitude  C40_True  \\\n",
       "22487       -47.882166        -14.235004         -51.925280         0   \n",
       "16354       -49.264693        -14.235004         -51.925280         1   \n",
       "192378      -81.034800         37.090240         -95.712891         0   \n",
       "70523      -111.631100         37.090240         -95.712891         0   \n",
       "97924      -106.837000         37.090240         -95.712891         0   \n",
       "49690       -84.512000         37.090240         -95.712891         0   \n",
       "185831      -81.694400         37.090240         -95.712891         0   \n",
       "273960       -3.179090         55.378051          -3.435973         0   \n",
       "88928      -122.156077         37.090240         -95.712891         0   \n",
       "218024     -111.631100         37.090240         -95.712891         0   \n",
       "\n",
       "        C40_False      Continent  \n",
       "22487           1  South America  \n",
       "16354           0  South America  \n",
       "192378          1  North America  \n",
       "70523           1  North America  \n",
       "97924           1  North America  \n",
       "49690           1  North America  \n",
       "185831          1  North America  \n",
       "273960          1         Europe  \n",
       "88928           1  North America  \n",
       "218024          1  North America  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7eab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ce03091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     13408\n",
       "City                                        13408\n",
       "AQI Value                                   13408\n",
       "AQI Category                                13408\n",
       "CO AQI Value                                13408\n",
       "CO AQI Category                             13408\n",
       "Ozone AQI Value                             13408\n",
       "Ozone AQI Category                          13408\n",
       "NO2 AQI Value                               13408\n",
       "NO2 AQI Category                            13408\n",
       "PM2.5 AQI Value                             13408\n",
       "PM2.5 AQI Category                          13408\n",
       "Region                                      13408\n",
       "Reporting year                              13408\n",
       "Total emissions (metric tonnes CO2e)        13408\n",
       "Population                                  13408\n",
       "GDP                                         13408\n",
       "GDP Year                                    13408\n",
       "Average annual temperature (in Celsius)​    13408\n",
       "​Average altitude (m)                       13408\n",
       "​Land area (in square km)                   13408\n",
       "City Latitude                               13408\n",
       "City Longitude                              13408\n",
       "Country Latitude                            13408\n",
       "Country Longitude                           13408\n",
       "C40_True                                    13408\n",
       "C40_False                                   13408\n",
       "Continent                                   13407\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\") # save df to a pickle file so it can be used for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.feature_selection import RFECV\n",
    "\n",
    "    # Load your DataFrame here\n",
    "    # For example: df = pd.read_csv('your_data.csv')\n",
    "    # Make sure to replace this with your actual data loading code\n",
    "\n",
    "    # Separate the features and the target\n",
    "    X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "    y = df['C40_True']\n",
    "\n",
    "    # Identifying numeric and categorical features\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Preprocessing pipelines for both numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    # Feature selection integrated within the classifier pipeline\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('feature_selection', RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')),\n",
    "                            ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "    # Hyperparameter tuning setup for the classifier after feature selection\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "        # Add more parameters here if needed\n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Fit GridSearchCV to find the best model\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best hyperparameters and score\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the best model\n",
    "    joblib.dump(best_model, 'best_rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Load your DataFrame here\n",
    "\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "# Separate the features and the target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "# Hyperparameter tuning setup for the classifier after feature selection\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 150],  # Reduced number of options for speed\n",
    "    'classifier__max_depth': [None, 10],  # Simplified to speed up\n",
    "    # Simplify other parameters as needed\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with the pipeline\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to find the best model more efficiently\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Feature Importances\n",
    "if 'classifier' in best_model.named_steps:\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    features = numeric_features.tolist() + categorical_features.tolist()  # Adjust as necessary\n",
    "    feature_importance_dict = dict(zip(features, importances))\n",
    "    print(\"Feature importances:\", feature_importance_dict)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model_with_cv_and_regularization.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7637a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "df = ...\n",
    "\n",
    "# Drop 'C40_True' from the features since it's the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a full pipeline with both preprocessing and the classifier\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    # You can add more parameters here\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2086b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "# df = ...\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out()\n",
    "feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Extracting feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Display feature importances\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_path\n",
    "test_df2 = dataset_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure df is your DataFrame loaded with the data\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Mean F1-score from CV: {cv_scores.mean()} ± {cv_scores.std()}\")\n",
    "\n",
    "# Splitting data for final training and testing (optional, as cross-validation already evaluates the model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model on the entire dataset or the training set only\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names correctly after fitting the model\n",
    "try:\n",
    "    categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "except AttributeError:  # For sklearn versions prior to 0.24\n",
    "    categorical_feature_names = categorical_transformer.get_feature_names(categorical_features)\n",
    "feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Extracting and displaying feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score on the test set: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "\n",
    "# Drop 'C40_True' from the features since it's the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a full pipeline with both preprocessing and the classifier\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    # You can add more parameters here\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded correctly\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f805477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded with your data\n",
    "# This is just an example setup, replace df with your actual dataframe\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Saving the model for later use\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Placeholder for loading your dataset\n",
    "# Ensure to load your actual dataset here\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1, errors='ignore')\n",
    "y = df['C40_True']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit the pipeline to train a RandomForest model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = numeric_features.tolist() + list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out())\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Save the model and feature importances\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n",
    "\n",
    "\n",
    "print(feature_importances.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the data from float to int in order to use pandas to calculate the correlations\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "df_cleaned = numeric_df.dropna()\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_cleaned.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relationships between all numerical features\n",
    "sns.pairplot(df.select_dtypes(include=['float64', 'int64']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('AQI Value')\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.scatter(df['AQI Value'], df['PM2.5 AQI Value'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8399c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AQI Value'],  label='AQI Value', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PM2.5 AQI Value'],  label='PM2.5 AQI Value', norm_hist=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'AQI Value' for each country\n",
    "country_aqi_means = df.groupby('Country')['AQI Value'].mean()\n",
    "\n",
    "# Sorting the countries by AQI value for better visualization\n",
    "country_aqi_means = country_aqi_means.sort_values()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25)) \n",
    "plt.barh(country_aqi_means.index, country_aqi_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'PM2.5 AQI Value' for each country\n",
    "country_pm25_means = df.groupby('Country')['PM2.5 AQI Value'].mean()\n",
    "# Sorting the countries by PM2.5 AQI value for better visualization\n",
    "country_pm25_means = country_pm25_means.sort_values()\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25))\n",
    "plt.barh(country_pm25_means.index, country_pm25_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average PM2.5 AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average PM2.5 AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['AQI Value'].values.reshape(-1, 1)\n",
    "y = df['PM2.5 AQI Value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.xlabel('AQI Value')\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14117185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6895792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the subsets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of Linear Regression model\n",
    "myreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to our data\n",
    "myreg.fit(X_train, y_train)\n",
    "myreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calculated coefficients\n",
    "a = myreg.coef_\n",
    "b = myreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = myreg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e01d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Linear Regression \n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X_train, a*X_train + b, color='blue')\n",
    "plt.plot(X_test, y_predicted, color='orange')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = myreg.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be591e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'Country' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='Country', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into 5 clusters using the KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df['cluster'] = kmeans.predict(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'cluster' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='cluster', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(df_filtered[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df_filtered['cluster'] = kmeans.labels_\n",
    "\n",
    "# Analyze centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Centroids:\\n\", centroids)\n",
    "\n",
    "# Plotting clusters and centroids\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_filtered, x='AQI Value', y='PM2.5 AQI Value', hue='cluster', palette='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = df[['Country', 'Population']]\n",
    "\n",
    "# Grouping and aggregating population data by country\n",
    "population_by_country = population_data.groupby('Country')['Population'].sum().reset_index()\n",
    "\n",
    "# Creating a pivot table with 'Country' as index\n",
    "pivot_population = population_by_country.set_index('Country')\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data=pivot_population, cmap='YlGnBu', annot=True, fmt=',.0f', linewidths=.5)\n",
    "plt.title('Population by Country')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Country Location' column into separate longitude and latitude columns\n",
    "df[['Latitude', 'Longitude']] = df['Country Location'].str.strip('()').str.split(', ', expand=True).astype(float)\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "scatter_plot = go.Scatter3d(\n",
    "    x=df['Longitude'],\n",
    "    y=df['Latitude'],\n",
    "    z=df['Population year'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',                # Set color to an array/list of desired values\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting layout\n",
    "layout = go.Layout(\n",
    "    title='3D Population Map',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Country Longitude'),\n",
    "        yaxis=dict(title='Country Latitude'),\n",
    "        zaxis=dict(title='Population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combining data and layout into a figure\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c549f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Country'] == 'Russian Federation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe = df[df['Country'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_europe.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_europe.sample(n=500, replace=False, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a05116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "# Load the GeoJSON data from a local file\n",
    "with open(r\"C:\\Users\\chz\\Documents\\BI Exercise\\Datasæt\\Eumap.json\", 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and already correctly set up\n",
    "m = folium.Map(location=[df_sampled['Country Latitude'].mean(), df_sampled['Country Longitude'].mean()], zoom_start=3)\n",
    "\n",
    "# Add markers for each data point\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker([row['Country Latitude'], row['Country Longitude']], popup=row['Population']).add_to(m)\n",
    "\n",
    "# Add polygon overlays for countries using the loaded GeoJSON data\n",
    "folium.GeoJson(data=geojson_data).add_to(m)\n",
    "\n",
    "# Save and display the map\n",
    "m.save('map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c45914",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f2663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
