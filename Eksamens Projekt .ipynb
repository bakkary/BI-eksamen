{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8657e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fa15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa693206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c63edc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54e8b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288720, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a3c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                              0\n",
       "City                                                 1\n",
       "AQI Value                                            0\n",
       "AQI Category                                         0\n",
       "CO AQI Value                                         0\n",
       "CO AQI Category                                      0\n",
       "Ozone AQI Value                                      0\n",
       "Ozone AQI Category                                   0\n",
       "NO2 AQI Value                                        0\n",
       "NO2 AQI Category                                     0\n",
       "PM2.5 AQI Value                                      0\n",
       "PM2.5 AQI Category                                   0\n",
       "Account number                                       0\n",
       "Organization                                         0\n",
       "Region                                               0\n",
       "C40                                             237791\n",
       "Access                                               0\n",
       "Reporting year                                       0\n",
       "Accounting year                                      0\n",
       "Boundary                                             0\n",
       "Protocol                                             0\n",
       "Protocol column                                 114290\n",
       "Gases included                                  149132\n",
       "Total emissions (metric tonnes CO2e)             20569\n",
       "Total Scope 1 Emissions (metric tonnes CO2e)     57629\n",
       "Total Scope 2 Emissions (metric tonnes CO2e)     61107\n",
       "Comment                                         191619\n",
       "Increase/Decrease from last year                 16022\n",
       "Reason for increase/decrease in emissions        63341\n",
       "Population                                           0\n",
       "Population year                                      0\n",
       "GDP                                              60686\n",
       "GDP Currency                                     59014\n",
       "GDP Year                                         59463\n",
       "GDP Source                                       58750\n",
       "Average annual temperature (in Celsius)​          5396\n",
       "​Average altitude (m)                            20535\n",
       "​Land area (in square km)                         1562\n",
       "City Location                                        0\n",
       "Country Location                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c8ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values from the data frame\n",
    "df_merged = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c3ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     0\n",
       "City                                        0\n",
       "AQI Value                                   0\n",
       "AQI Category                                0\n",
       "CO AQI Value                                0\n",
       "CO AQI Category                             0\n",
       "Ozone AQI Value                             0\n",
       "Ozone AQI Category                          0\n",
       "NO2 AQI Value                               0\n",
       "NO2 AQI Category                            0\n",
       "PM2.5 AQI Value                             0\n",
       "PM2.5 AQI Category                          0\n",
       "Region                                      0\n",
       "C40                                         0\n",
       "Reporting year                              0\n",
       "Total emissions (metric tonnes CO2e)        0\n",
       "Population                                  0\n",
       "GDP                                         0\n",
       "GDP Year                                    0\n",
       "Average annual temperature (in Celsius)​    0\n",
       "​Average altitude (m)                       0\n",
       "​Land area (in square km)                   0\n",
       "City Location                               0\n",
       "Country Location                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data frame for null values\n",
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2328ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195254\n",
       "City                                        195254\n",
       "AQI Value                                   195254\n",
       "AQI Category                                195254\n",
       "CO AQI Value                                195254\n",
       "CO AQI Category                             195254\n",
       "Ozone AQI Value                             195254\n",
       "Ozone AQI Category                          195254\n",
       "NO2 AQI Value                               195254\n",
       "NO2 AQI Category                            195254\n",
       "PM2.5 AQI Value                             195254\n",
       "PM2.5 AQI Category                          195254\n",
       "Region                                      195254\n",
       "C40                                         195254\n",
       "Reporting year                              195254\n",
       "Total emissions (metric tonnes CO2e)        195254\n",
       "Population                                  195254\n",
       "GDP                                         195254\n",
       "GDP Year                                    195254\n",
       "Average annual temperature (in Celsius)​    195254\n",
       "​Average altitude (m)                       195254\n",
       "​Land area (in square km)                   195254\n",
       "City Location                               195254\n",
       "Country Location                            195254\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbf8382b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Location</th>\n",
       "      <th>Country Location</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>(-12.97304, -38.502304)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-12.97304</td>\n",
       "      <td>-38.502304</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.1</td>\n",
       "      <td>(-22.892857, -43.118381)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-22.892857</td>\n",
       "      <td>-43.118381</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>(-27.5949884, -48.5481743)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-27.5949884</td>\n",
       "      <td>-48.5481743</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>(-19.916681, -43.934493)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-19.916681</td>\n",
       "      <td>-43.934493</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>749.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>(-16.6868912, -49.2647943)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-16.6868912</td>\n",
       "      <td>-49.2647943</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country              City  AQI Value AQI Category  CO AQI Value  \\\n",
       "1241  Brazil  Presidente Dutra         41         Good             1   \n",
       "1242  Brazil  Presidente Dutra         41         Good             1   \n",
       "1243  Brazil  Presidente Dutra         41         Good             1   \n",
       "1244  Brazil  Presidente Dutra         41         Good             1   \n",
       "1246  Brazil  Presidente Dutra         41         Good             1   \n",
       "\n",
       "     CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "1241            Good                5               Good              1   \n",
       "1242            Good                5               Good              1   \n",
       "1243            Good                5               Good              1   \n",
       "1244            Good                5               Good              1   \n",
       "1246            Good                5               Good              1   \n",
       "\n",
       "     NO2 AQI Category  ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "1241             Good  ...    2012.0                                     26.0   \n",
       "1242             Good  ...    2013.0                                     23.0   \n",
       "1243             Good  ...    2013.0                                     20.0   \n",
       "1244             Good  ...    2014.0                                     21.0   \n",
       "1246             Good  ...    2010.0                                     23.2   \n",
       "\n",
       "     ​Average altitude (m) ​Land area (in square km)  \\\n",
       "1241                   8.0                     692.0   \n",
       "1242                   5.0                     133.1   \n",
       "1243                   3.0                     438.0   \n",
       "1244                 900.0                     331.0   \n",
       "1246                 749.0                     739.0   \n",
       "\n",
       "                   City Location         Country Location  City Latitude  \\\n",
       "1241     (-12.97304, -38.502304)  (-14.235004, -51.92528)      -12.97304   \n",
       "1242    (-22.892857, -43.118381)  (-14.235004, -51.92528)     -22.892857   \n",
       "1243  (-27.5949884, -48.5481743)  (-14.235004, -51.92528)    -27.5949884   \n",
       "1244    (-19.916681, -43.934493)  (-14.235004, -51.92528)     -19.916681   \n",
       "1246  (-16.6868912, -49.2647943)  (-14.235004, -51.92528)    -16.6868912   \n",
       "\n",
       "      City Longitude  Country Latitude  Country Longitude  \n",
       "1241      -38.502304        -14.235004          -51.92528  \n",
       "1242      -43.118381        -14.235004          -51.92528  \n",
       "1243     -48.5481743        -14.235004          -51.92528  \n",
       "1244      -43.934493        -14.235004          -51.92528  \n",
       "1246     -49.2647943        -14.235004          -51.92528  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5b9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b08ec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c658a0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58e5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b97ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf32c49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Nova Friburgo</td>\n",
       "      <td>80</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>13</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1224.6</td>\n",
       "      <td>-22.905439</td>\n",
       "      <td>-43.561447</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country           City  AQI Value AQI Category  CO AQI Value  \\\n",
       "7169  Brazil  Nova Friburgo         80     Moderate             2   \n",
       "\n",
       "     CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "7169            Good               13               Good              6   \n",
       "\n",
       "     NO2 AQI Category  ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "7169             Good  ...    2014.0                                     23.7   \n",
       "\n",
       "     ​Average altitude (m)  ​Land area (in square km)  City Latitude  \\\n",
       "7169                   5.0                     1224.6     -22.905439   \n",
       "\n",
       "      City Longitude  Country Latitude  Country Longitude  C40_True  C40_False  \n",
       "7169      -43.561447        -14.235004          -51.92528         1          0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba503efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195254 entries, 1241 to 288719\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Country                                   195254 non-null  object \n",
      " 1   City                                      195254 non-null  object \n",
      " 2   AQI Value                                 195254 non-null  int64  \n",
      " 3   AQI Category                              195254 non-null  object \n",
      " 4   CO AQI Value                              195254 non-null  int64  \n",
      " 5   CO AQI Category                           195254 non-null  object \n",
      " 6   Ozone AQI Value                           195254 non-null  int64  \n",
      " 7   Ozone AQI Category                        195254 non-null  object \n",
      " 8   NO2 AQI Value                             195254 non-null  int64  \n",
      " 9   NO2 AQI Category                          195254 non-null  object \n",
      " 10  PM2.5 AQI Value                           195254 non-null  int64  \n",
      " 11  PM2.5 AQI Category                        195254 non-null  object \n",
      " 12  Region                                    195254 non-null  object \n",
      " 13  Reporting year                            195254 non-null  int64  \n",
      " 14  Total emissions (metric tonnes CO2e)      195254 non-null  float64\n",
      " 15  Population                                195254 non-null  int64  \n",
      " 16  GDP                                       195254 non-null  float64\n",
      " 17  GDP Year                                  195254 non-null  float64\n",
      " 18  Average annual temperature (in Celsius)​  195254 non-null  float64\n",
      " 19  ​Average altitude (m)                     195254 non-null  float64\n",
      " 20  ​Land area (in square km)                 195254 non-null  float64\n",
      " 21  City Latitude                             195254 non-null  float64\n",
      " 22  City Longitude                            195254 non-null  float64\n",
      " 23  Country Latitude                          195254 non-null  float64\n",
      " 24  Country Longitude                         195254 non-null  float64\n",
      " 25  C40_True                                  195254 non-null  int32  \n",
      " 26  C40_False                                 195254 non-null  int32  \n",
      "dtypes: float64(10), int32(2), int64(7), object(8)\n",
      "memory usage: 40.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761cee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's official_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\pycountry\\db.py:51: UserWarning: Country's common_name not found. Country name provided instead.\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16c832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195254\n",
       "City                                        195254\n",
       "AQI Value                                   195254\n",
       "AQI Category                                195254\n",
       "CO AQI Value                                195254\n",
       "CO AQI Category                             195254\n",
       "Ozone AQI Value                             195254\n",
       "Ozone AQI Category                          195254\n",
       "NO2 AQI Value                               195254\n",
       "NO2 AQI Category                            195254\n",
       "PM2.5 AQI Value                             195254\n",
       "PM2.5 AQI Category                          195254\n",
       "Region                                      195254\n",
       "Reporting year                              195254\n",
       "Total emissions (metric tonnes CO2e)        195254\n",
       "Population                                  195254\n",
       "GDP                                         195254\n",
       "GDP Year                                    195254\n",
       "Average annual temperature (in Celsius)​    195254\n",
       "​Average altitude (m)                       195254\n",
       "​Land area (in square km)                   195254\n",
       "City Latitude                               195254\n",
       "City Longitude                              195254\n",
       "Country Latitude                            195254\n",
       "Country Longitude                           195254\n",
       "C40_True                                    195254\n",
       "C40_False                                   195254\n",
       "Continent                                   195253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e50160f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "      <th>Continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273467</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Alfreton</td>\n",
       "      <td>40</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>33</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>51.504858</td>\n",
       "      <td>-0.078689</td>\n",
       "      <td>55.378051</td>\n",
       "      <td>-3.435973</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95131</th>\n",
       "      <td>USA</td>\n",
       "      <td>Laguna Hills</td>\n",
       "      <td>62</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>25</td>\n",
       "      <td>Good</td>\n",
       "      <td>12</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>39.103100</td>\n",
       "      <td>-84.512000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132864</th>\n",
       "      <td>USA</td>\n",
       "      <td>Merritt Island</td>\n",
       "      <td>48</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>26</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>203.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>41.661100</td>\n",
       "      <td>-91.530200</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120212</th>\n",
       "      <td>USA</td>\n",
       "      <td>Lake Oswego</td>\n",
       "      <td>74</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>24</td>\n",
       "      <td>Good</td>\n",
       "      <td>7</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>20.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>29.951100</td>\n",
       "      <td>-90.071500</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252514</th>\n",
       "      <td>USA</td>\n",
       "      <td>Moline</td>\n",
       "      <td>77</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>3</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>10</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>39.103100</td>\n",
       "      <td>-84.512000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220558</th>\n",
       "      <td>USA</td>\n",
       "      <td>Drexel Hill</td>\n",
       "      <td>45</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>42</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.195000</td>\n",
       "      <td>-106.837000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82024</th>\n",
       "      <td>USA</td>\n",
       "      <td>Durango</td>\n",
       "      <td>500</td>\n",
       "      <td>Hazardous</td>\n",
       "      <td>133</td>\n",
       "      <td>Unhealthy for Sensitive Groups</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>53</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>...</td>\n",
       "      <td>14.3</td>\n",
       "      <td>89.0</td>\n",
       "      <td>21.8</td>\n",
       "      <td>37.379700</td>\n",
       "      <td>-122.137500</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167993</th>\n",
       "      <td>USA</td>\n",
       "      <td>Tifton</td>\n",
       "      <td>85</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>Good</td>\n",
       "      <td>9</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>30.267200</td>\n",
       "      <td>-97.743100</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13446</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Mutuipe</td>\n",
       "      <td>21</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>11</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>-8.057838</td>\n",
       "      <td>-34.882897</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.925280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248858</th>\n",
       "      <td>USA</td>\n",
       "      <td>Waterford</td>\n",
       "      <td>47</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>32</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>23.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>378.1</td>\n",
       "      <td>25.901700</td>\n",
       "      <td>-97.497500</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Country            City  AQI Value AQI Category  CO AQI Value  \\\n",
       "273467  United Kingdom        Alfreton         40         Good             1   \n",
       "95131              USA    Laguna Hills         62     Moderate             1   \n",
       "132864             USA  Merritt Island         48         Good             1   \n",
       "120212             USA     Lake Oswego         74     Moderate             2   \n",
       "252514             USA          Moline         77     Moderate             3   \n",
       "220558             USA     Drexel Hill         45         Good             1   \n",
       "82024              USA         Durango        500    Hazardous           133   \n",
       "167993             USA          Tifton         85     Moderate             2   \n",
       "13446           Brazil         Mutuipe         21         Good             0   \n",
       "248858             USA       Waterford         47         Good             1   \n",
       "\n",
       "                       CO AQI Category  Ozone AQI Value Ozone AQI Category  \\\n",
       "273467                            Good               33               Good   \n",
       "95131                             Good               25               Good   \n",
       "132864                            Good               26               Good   \n",
       "120212                            Good               24               Good   \n",
       "252514                            Good                1               Good   \n",
       "220558                            Good               42               Good   \n",
       "82024   Unhealthy for Sensitive Groups                0               Good   \n",
       "167993                            Good                8               Good   \n",
       "13446                             Good               11               Good   \n",
       "248858                            Good               32               Good   \n",
       "\n",
       "        NO2 AQI Value NO2 AQI Category  ...  \\\n",
       "273467              1             Good  ...   \n",
       "95131              12             Good  ...   \n",
       "132864              5             Good  ...   \n",
       "120212              7             Good  ...   \n",
       "252514             10             Good  ...   \n",
       "220558              2             Good  ...   \n",
       "82024              53         Moderate  ...   \n",
       "167993              9             Good  ...   \n",
       "13446               1             Good  ...   \n",
       "248858              6             Good  ...   \n",
       "\n",
       "        Average annual temperature (in Celsius)​ ​Average altitude (m)  \\\n",
       "273467                                      11.0                  35.0   \n",
       "95131                                       13.0                 168.0   \n",
       "132864                                      10.5                 203.0   \n",
       "120212                                      20.9                  -1.0   \n",
       "252514                                      13.0                 168.0   \n",
       "220558                                       6.0                2405.0   \n",
       "82024                                       14.3                  89.0   \n",
       "167993                                      20.0                 149.0   \n",
       "13446                                       26.0                   4.0   \n",
       "248858                                      23.6                  11.0   \n",
       "\n",
       "       ​Land area (in square km)  City Latitude  City Longitude  \\\n",
       "273467                    1572.0      51.504858       -0.078689   \n",
       "95131                      124.0      39.103100      -84.512000   \n",
       "132864                      65.5      41.661100      -91.530200   \n",
       "120212                     468.0      29.951100      -90.071500   \n",
       "252514                     124.0      39.103100      -84.512000   \n",
       "220558                      20.0      39.195000     -106.837000   \n",
       "82024                       21.8      37.379700     -122.137500   \n",
       "167993                     704.0      30.267200      -97.743100   \n",
       "13446                      218.0      -8.057838      -34.882897   \n",
       "248858                     378.1      25.901700      -97.497500   \n",
       "\n",
       "        Country Latitude  Country Longitude  C40_True  C40_False  \\\n",
       "273467         55.378051          -3.435973         1          0   \n",
       "95131          37.090240         -95.712891         0          1   \n",
       "132864         37.090240         -95.712891         0          1   \n",
       "120212         37.090240         -95.712891         1          0   \n",
       "252514         37.090240         -95.712891         0          1   \n",
       "220558         37.090240         -95.712891         0          1   \n",
       "82024          37.090240         -95.712891         0          1   \n",
       "167993         37.090240         -95.712891         1          0   \n",
       "13446         -14.235004         -51.925280         0          1   \n",
       "248858         37.090240         -95.712891         0          1   \n",
       "\n",
       "            Continent  \n",
       "273467         Europe  \n",
       "95131   North America  \n",
       "132864  North America  \n",
       "120212  North America  \n",
       "252514  North America  \n",
       "220558  North America  \n",
       "82024   North America  \n",
       "167993  North America  \n",
       "13446   South America  \n",
       "248858  North America  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7eab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ce03091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     13412\n",
       "City                                        13412\n",
       "AQI Value                                   13412\n",
       "AQI Category                                13412\n",
       "CO AQI Value                                13412\n",
       "CO AQI Category                             13412\n",
       "Ozone AQI Value                             13412\n",
       "Ozone AQI Category                          13412\n",
       "NO2 AQI Value                               13412\n",
       "NO2 AQI Category                            13412\n",
       "PM2.5 AQI Value                             13412\n",
       "PM2.5 AQI Category                          13412\n",
       "Region                                      13412\n",
       "Reporting year                              13412\n",
       "Total emissions (metric tonnes CO2e)        13412\n",
       "Population                                  13412\n",
       "GDP                                         13412\n",
       "GDP Year                                    13412\n",
       "Average annual temperature (in Celsius)​    13412\n",
       "​Average altitude (m)                       13412\n",
       "​Land area (in square km)                   13412\n",
       "City Latitude                               13412\n",
       "City Longitude                              13412\n",
       "Country Latitude                            13412\n",
       "Country Longitude                           13412\n",
       "C40_True                                    13412\n",
       "C40_False                                   13412\n",
       "Continent                                   13411\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23589654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.isna().any()]\n",
    "df.loc[288719]\n",
    "df.fillna(value=\"Europe\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\") # save df to a pickle file so it can be used for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65628641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__n_estimators': 100, 'classifier__max_depth': None}\n",
      "Best cross-validation score: 0.97\n",
      "Test set accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1086\n",
      "           1       1.00      1.00      1.00      1597\n",
      "\n",
      "    accuracy                           1.00      2683\n",
      "   macro avg       1.00      1.00      1.00      2683\n",
      "weighted avg       1.00      1.00      1.00      2683\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Load your DataFrame here\n",
    "\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "# Separate the features and the target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "# Hyperparameter tuning setup for the classifier after feature selection\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 150],  # Reduced number of options for speed\n",
    "    'classifier__max_depth': [None, 10],  # Simplified to speed up\n",
    "    # Simplify other parameters as needed\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with the pipeline\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to find the best model more efficiently\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39fb56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters: {'classifier__n_estimators': 100, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 10}\n",
      "Best score: 0.9724086502609992\n",
      "Feature importances: {'AQI Value': 0.0011847060872996423, 'CO AQI Value': 0.001354552911399358, 'Ozone AQI Value': 0.001653299875194524, 'NO2 AQI Value': 0.0014820535475479823, 'PM2.5 AQI Value': 0.002216516210907841, 'Reporting year': 0.0, 'Total emissions (metric tonnes CO2e)': 0.14410920591934948, 'Population': 0.24013009792530077, 'GDP': 0.06168798208898982, 'GDP Year': 0.02995889929529848, 'Average annual temperature (in Celsius)\\u200b': 0.04594427138631429, '\\u200bAverage altitude (m)': 0.05695925967169357, '\\u200bLand area (in square km)': 0.04589500857228878, 'City Latitude': 0.058289225378001824, 'City Longitude': 0.08973002002907877, 'Country Latitude': 0.056923928684978205, 'Country Longitude': 0.11033552237536101, 'AQI Category': 0.0013020841002100913, 'CO AQI Category': 0.0, 'Ozone AQI Category': 4.058043858879416e-06, 'NO2 AQI Category': 0.00010180823282131039, 'PM2.5 AQI Category': 1.67585890646827e-06, 'Region': 6.668654109616679e-05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model_with_cv_and_regularization.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Feature Importances\n",
    "if 'classifier' in best_model.named_steps:\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    features = numeric_features.tolist() + categorical_features.tolist()  # Adjust as necessary\n",
    "    feature_importance_dict = dict(zip(features, importances))\n",
    "    print(\"Feature importances:\", feature_importance_dict)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model_with_cv_and_regularization.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2086b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Extracting feature names\u001b[39;00m\n\u001b[0;32m     53\u001b[0m numeric_feature_names \u001b[38;5;241m=\u001b[39m numeric_features\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    409\u001b[0m     cloned_transformer,\n\u001b[0;32m    410\u001b[0m     X,\n\u001b[0;32m    411\u001b[0m     y,\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    414\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    415\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:774\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# Re-execute an elimination with best_k over the whole set\u001b[39;00m\n\u001b[0;32m    766\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(\n\u001b[0;32m    767\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator,\n\u001b[0;32m    768\u001b[0m     n_features_to_select\u001b[38;5;241m=\u001b[39mn_features_to_select,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    772\u001b[0m )\n\u001b[1;32m--> 774\u001b[0m rfe\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Set final attributes\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_ \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39msupport_\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[0;32m   1297\u001b[0m     path_func(\n\u001b[0;32m   1298\u001b[0m         X,\n\u001b[0;32m   1299\u001b[0m         y,\n\u001b[0;32m   1300\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[0;32m   1301\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[0;32m   1302\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[0;32m   1303\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1304\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1305\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1306\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[0;32m   1307\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[0;32m   1308\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1309\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1310\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1311\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1312\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[0;32m   1313\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[0;32m   1314\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[0;32m   1315\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1316\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m   1317\u001b[0m     )\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[0;32m   1319\u001b[0m )\n\u001b[0;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[0;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    454\u001b[0m ]\n\u001b[1;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    456\u001b[0m     func,\n\u001b[0;32m    457\u001b[0m     w0,\n\u001b[0;32m    458\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    459\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    460\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    461\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_iter,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# default is 20\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: iprint,\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: tol,\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    467\u001b[0m     },\n\u001b[0;32m    468\u001b[0m )\n\u001b[0;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    470\u001b[0m     solver,\n\u001b[0;32m    471\u001b[0m     opt_res,\n\u001b[0;32m    472\u001b[0m     max_iter,\n\u001b[0;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:280\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[1;32m--> 280\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_loss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    281\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    282\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    283\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    284\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    286\u001b[0m sw_sum \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(sample_weight)\n\u001b[0;32m    287\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m sw_sum\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\_loss\\loss.py:254\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[1;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    252\u001b[0m     gradient_out \u001b[38;5;241m=\u001b[39m gradient_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    255\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my_true,\n\u001b[0;32m    256\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    257\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    258\u001b[0m     loss_out\u001b[38;5;241m=\u001b[39mloss_out,\n\u001b[0;32m    259\u001b[0m     gradient_out\u001b[38;5;241m=\u001b[39mgradient_out,\n\u001b[0;32m    260\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    261\u001b[0m )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_out, gradient_out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "# df = ...\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out()\n",
    "feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Extracting feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Display feature importances\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7f7c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_path\n",
    "test_df2 = dataset_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2998b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-score from CV: 0.9587882580957764 ± 0.08242348380844709\n",
      "                                Feature  Importance\n",
      "0                         NO2 AQI Value    0.298753\n",
      "1                       Ozone AQI Value    0.231453\n",
      "2                              GDP Year    0.109527\n",
      "3                 ​Average altitude (m)    0.091924\n",
      "4  Total emissions (metric tonnes CO2e)    0.059157\n",
      "Model F1-score on the test set: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1086\n",
      "           1       1.00      1.00      1.00      1597\n",
      "\n",
      "    accuracy                           1.00      2683\n",
      "   macro avg       1.00      1.00      1.00      2683\n",
      "weighted avg       1.00      1.00      1.00      2683\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature_importances.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure df is your DataFrame loaded with the data\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Mean F1-score from CV: {cv_scores.mean()} ± {cv_scores.std()}\")\n",
    "\n",
    "# Splitting data for final training and testing (optional, as cross-validation already evaluates the model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model on the entire dataset or the training set only\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names correctly after fitting the model\n",
    "try:\n",
    "    categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "except AttributeError:  # For sklearn versions prior to 0.24\n",
    "    categorical_feature_names = categorical_transformer.get_feature_names(categorical_features)\n",
    "feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Extracting and displaying feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score on the test set: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d184f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'randomforestclassifier__max_depth': None, 'randomforestclassifier__n_estimators': 200}\n",
      "Best cross-validation score: 0.97\n",
      "Test set accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1086\n",
      "           1       1.00      1.00      1.00      1597\n",
      "\n",
      "    accuracy                           1.00      2683\n",
      "   macro avg       1.00      1.00      1.00      2683\n",
      "weighted avg       1.00      1.00      1.00      2683\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "\n",
    "# Drop 'C40_True' from the features since it's the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a full pipeline with both preprocessing and the classifier\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    # You can add more parameters here\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0304823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [1.         1.         1.         1.         0.79567487]\n",
      "Mean Accuracy: 0.9591349739000746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['less_overfitting_model.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded correctly\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f805477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [1.         1.         1.         1.         0.79567487]\n",
      "Mean Accuracy: 0.9591349739000746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['less_overfitting_model.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded with your data\n",
    "# This is just an example setup, replace df with your actual dataframe\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Saving the model for later use\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Placeholder for loading your dataset\n",
    "# Ensure to load your actual dataset here\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1, errors='ignore')\n",
    "y = df['C40_True']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit the pipeline to train a RandomForest model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = numeric_features.tolist() + list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out())\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Save the model and feature importances\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n",
    "\n",
    "\n",
    "print(feature_importances.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the data from float to int in order to use pandas to calculate the correlations\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "df_cleaned = numeric_df.dropna()\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_cleaned.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relationships between all numerical features\n",
    "sns.pairplot(df.select_dtypes(include=['float64', 'int64']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('AQI Value')\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.scatter(df['AQI Value'], df['PM2.5 AQI Value'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8399c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AQI Value'],  label='AQI Value', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PM2.5 AQI Value'],  label='PM2.5 AQI Value', norm_hist=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'AQI Value' for each country\n",
    "country_aqi_means = df.groupby('Country')['AQI Value'].mean()\n",
    "\n",
    "# Sorting the countries by AQI value for better visualization\n",
    "country_aqi_means = country_aqi_means.sort_values()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25)) \n",
    "plt.barh(country_aqi_means.index, country_aqi_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'PM2.5 AQI Value' for each country\n",
    "country_pm25_means = df.groupby('Country')['PM2.5 AQI Value'].mean()\n",
    "# Sorting the countries by PM2.5 AQI value for better visualization\n",
    "country_pm25_means = country_pm25_means.sort_values()\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25))\n",
    "plt.barh(country_pm25_means.index, country_pm25_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average PM2.5 AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average PM2.5 AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['AQI Value'].values.reshape(-1, 1)\n",
    "y = df['PM2.5 AQI Value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.xlabel('AQI Value')\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14117185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6895792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the subsets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of Linear Regression model\n",
    "myreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to our data\n",
    "myreg.fit(X_train, y_train)\n",
    "myreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calculated coefficients\n",
    "a = myreg.coef_\n",
    "b = myreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = myreg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e01d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Linear Regression \n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X_train, a*X_train + b, color='blue')\n",
    "plt.plot(X_test, y_predicted, color='orange')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = myreg.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be591e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'Country' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='Country', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into 5 clusters using the KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df['cluster'] = kmeans.predict(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'cluster' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='cluster', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(df_filtered[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df_filtered['cluster'] = kmeans.labels_\n",
    "\n",
    "# Analyze centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Centroids:\\n\", centroids)\n",
    "\n",
    "# Plotting clusters and centroids\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_filtered, x='AQI Value', y='PM2.5 AQI Value', hue='cluster', palette='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = df[['Country', 'Population']]\n",
    "\n",
    "# Grouping and aggregating population data by country\n",
    "population_by_country = population_data.groupby('Country')['Population'].sum().reset_index()\n",
    "\n",
    "# Creating a pivot table with 'Country' as index\n",
    "pivot_population = population_by_country.set_index('Country')\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data=pivot_population, cmap='YlGnBu', annot=True, fmt=',.0f', linewidths=.5)\n",
    "plt.title('Population by Country')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Country Location' column into separate longitude and latitude columns\n",
    "df[['Latitude', 'Longitude']] = df['Country Location'].str.strip('()').str.split(', ', expand=True).astype(float)\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "scatter_plot = go.Scatter3d(\n",
    "    x=df['Longitude'],\n",
    "    y=df['Latitude'],\n",
    "    z=df['Population year'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',                # Set color to an array/list of desired values\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting layout\n",
    "layout = go.Layout(\n",
    "    title='3D Population Map',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Country Longitude'),\n",
    "        yaxis=dict(title='Country Latitude'),\n",
    "        zaxis=dict(title='Population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combining data and layout into a figure\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c549f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Country'] == 'Russian Federation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe = df[df['Country'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_europe.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_europe.sample(n=500, replace=False, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a05116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "# Load the GeoJSON data from a local file\n",
    "with open(r\"C:\\Users\\chz\\Documents\\BI Exercise\\Datasæt\\Eumap.json\", 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and already correctly set up\n",
    "m = folium.Map(location=[df_sampled['Country Latitude'].mean(), df_sampled['Country Longitude'].mean()], zoom_start=3)\n",
    "\n",
    "# Add markers for each data point\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker([row['Country Latitude'], row['Country Longitude']], popup=row['Population']).add_to(m)\n",
    "\n",
    "# Add polygon overlays for countries using the loaded GeoJSON data\n",
    "folium.GeoJson(data=geojson_data).add_to(m)\n",
    "\n",
    "# Save and display the map\n",
    "m.save('map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c45914",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f2663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
