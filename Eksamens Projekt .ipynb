{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97efdf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\tobia\\anaconda3\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from plotly) (8.2.2)\n",
      "^C\n",
      "Requirement already satisfied: Streamlit in c:\\users\\tobia\\anaconda3\\lib\\site-packages (1.31.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (5.2.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (6.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (1.24.3)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (13.7.0)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (4.7.1)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (5.2)\n",
      "Requirement already satisfied: validators<1,>=0.2 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (0.22.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (3.1.42)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (0.8.1b0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (6.3.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from Streamlit) (2.1.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->Streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->Streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->Streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from click<9,>=7.0->Streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->Streamlit) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from importlib-metadata<8,>=1.4->Streamlit) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->Streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->Streamlit) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.3->Streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->Streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->Streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->Streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->Streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->Streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->Streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->Streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->Streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->Streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->Streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->Streamlit) (0.1.0)\n",
      "Requirement already satisfied: folium in c:\\users\\tobia\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from folium) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from folium) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from folium) (2.31.0)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from folium) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests->folium) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tobia\\anaconda3\\lib\\site-packages (from requests->folium) (2024.2.2)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\tobia\\anaconda3\\lib\\site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8657e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fa15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa693206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c63edc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54e8b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288720, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c8ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values from the data frame\n",
    "df_merged = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c3ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     0\n",
       "City                                        0\n",
       "AQI Value                                   0\n",
       "AQI Category                                0\n",
       "CO AQI Value                                0\n",
       "CO AQI Category                             0\n",
       "Ozone AQI Value                             0\n",
       "Ozone AQI Category                          0\n",
       "NO2 AQI Value                               0\n",
       "NO2 AQI Category                            0\n",
       "PM2.5 AQI Value                             0\n",
       "PM2.5 AQI Category                          0\n",
       "Region                                      0\n",
       "C40                                         0\n",
       "Reporting year                              0\n",
       "Total emissions (metric tonnes CO2e)        0\n",
       "Population                                  0\n",
       "GDP                                         0\n",
       "GDP Year                                    0\n",
       "Average annual temperature (in Celsius)​    0\n",
       "​Average altitude (m)                       0\n",
       "​Land area (in square km)                   0\n",
       "City Location                               0\n",
       "Country Location                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data frame for null values\n",
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2328ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf8382b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Location</th>\n",
       "      <th>Country Location</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>(-12.97304, -38.502304)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-12.97304</td>\n",
       "      <td>-38.502304</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.1</td>\n",
       "      <td>(-22.892857, -43.118381)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-22.892857</td>\n",
       "      <td>-43.118381</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>(-27.5949884, -48.5481743)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-27.5949884</td>\n",
       "      <td>-48.5481743</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>(-19.916681, -43.934493)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-19.916681</td>\n",
       "      <td>-43.934493</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>749.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>(-16.6868912, -49.2647943)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-16.6868912</td>\n",
       "      <td>-49.2647943</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country              City  AQI Value AQI Category  CO AQI Value  \\\n",
       "1241  Brazil  Presidente Dutra         41         Good             1   \n",
       "1242  Brazil  Presidente Dutra         41         Good             1   \n",
       "1243  Brazil  Presidente Dutra         41         Good             1   \n",
       "1244  Brazil  Presidente Dutra         41         Good             1   \n",
       "1246  Brazil  Presidente Dutra         41         Good             1   \n",
       "\n",
       "     CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "1241            Good                5               Good              1   \n",
       "1242            Good                5               Good              1   \n",
       "1243            Good                5               Good              1   \n",
       "1244            Good                5               Good              1   \n",
       "1246            Good                5               Good              1   \n",
       "\n",
       "     NO2 AQI Category  ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "1241             Good  ...    2012.0                                     26.0   \n",
       "1242             Good  ...    2013.0                                     23.0   \n",
       "1243             Good  ...    2013.0                                     20.0   \n",
       "1244             Good  ...    2014.0                                     21.0   \n",
       "1246             Good  ...    2010.0                                     23.2   \n",
       "\n",
       "     ​Average altitude (m) ​Land area (in square km)  \\\n",
       "1241                   8.0                     692.0   \n",
       "1242                   5.0                     133.1   \n",
       "1243                   3.0                     438.0   \n",
       "1244                 900.0                     331.0   \n",
       "1246                 749.0                     739.0   \n",
       "\n",
       "                   City Location         Country Location  City Latitude  \\\n",
       "1241     (-12.97304, -38.502304)  (-14.235004, -51.92528)      -12.97304   \n",
       "1242    (-22.892857, -43.118381)  (-14.235004, -51.92528)     -22.892857   \n",
       "1243  (-27.5949884, -48.5481743)  (-14.235004, -51.92528)    -27.5949884   \n",
       "1244    (-19.916681, -43.934493)  (-14.235004, -51.92528)     -19.916681   \n",
       "1246  (-16.6868912, -49.2647943)  (-14.235004, -51.92528)    -16.6868912   \n",
       "\n",
       "      City Longitude  Country Latitude  Country Longitude  \n",
       "1241      -38.502304        -14.235004          -51.92528  \n",
       "1242      -43.118381        -14.235004          -51.92528  \n",
       "1243     -48.5481743        -14.235004          -51.92528  \n",
       "1244      -43.934493        -14.235004          -51.92528  \n",
       "1246     -49.2647943        -14.235004          -51.92528  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5b9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08ec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c658a0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a58e5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b97ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32c49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba503efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195254 entries, 1241 to 288719\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Country                                   195254 non-null  object \n",
      " 1   City                                      195254 non-null  object \n",
      " 2   AQI Value                                 195254 non-null  int64  \n",
      " 3   AQI Category                              195254 non-null  object \n",
      " 4   CO AQI Value                              195254 non-null  int64  \n",
      " 5   CO AQI Category                           195254 non-null  object \n",
      " 6   Ozone AQI Value                           195254 non-null  int64  \n",
      " 7   Ozone AQI Category                        195254 non-null  object \n",
      " 8   NO2 AQI Value                             195254 non-null  int64  \n",
      " 9   NO2 AQI Category                          195254 non-null  object \n",
      " 10  PM2.5 AQI Value                           195254 non-null  int64  \n",
      " 11  PM2.5 AQI Category                        195254 non-null  object \n",
      " 12  Region                                    195254 non-null  object \n",
      " 13  Reporting year                            195254 non-null  int64  \n",
      " 14  Total emissions (metric tonnes CO2e)      195254 non-null  float64\n",
      " 15  Population                                195254 non-null  int64  \n",
      " 16  GDP                                       195254 non-null  float64\n",
      " 17  GDP Year                                  195254 non-null  float64\n",
      " 18  Average annual temperature (in Celsius)​  195254 non-null  float64\n",
      " 19  ​Average altitude (m)                     195254 non-null  float64\n",
      " 20  ​Land area (in square km)                 195254 non-null  float64\n",
      " 21  City Latitude                             195254 non-null  float64\n",
      " 22  City Longitude                            195254 non-null  float64\n",
      " 23  Country Latitude                          195254 non-null  float64\n",
      " 24  Country Longitude                         195254 non-null  float64\n",
      " 25  C40_True                                  195254 non-null  int32  \n",
      " 26  C40_False                                 195254 non-null  int32  \n",
      "dtypes: float64(10), int32(2), int64(7), object(8)\n",
      "memory usage: 40.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "761cee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16c832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195254\n",
       "City                                        195254\n",
       "AQI Value                                   195254\n",
       "AQI Category                                195254\n",
       "CO AQI Value                                195254\n",
       "CO AQI Category                             195254\n",
       "Ozone AQI Value                             195254\n",
       "Ozone AQI Category                          195254\n",
       "NO2 AQI Value                               195254\n",
       "NO2 AQI Category                            195254\n",
       "PM2.5 AQI Value                             195254\n",
       "PM2.5 AQI Category                          195254\n",
       "Region                                      195254\n",
       "Reporting year                              195254\n",
       "Total emissions (metric tonnes CO2e)        195254\n",
       "Population                                  195254\n",
       "GDP                                         195254\n",
       "GDP Year                                    195254\n",
       "Average annual temperature (in Celsius)​    195254\n",
       "​Average altitude (m)                       195254\n",
       "​Land area (in square km)                   195254\n",
       "City Latitude                               195254\n",
       "City Longitude                              195254\n",
       "Country Latitude                            195254\n",
       "Country Longitude                           195254\n",
       "C40_True                                    195254\n",
       "C40_False                                   195254\n",
       "Continent                                   195253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50160f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7eab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ce03091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195254\n",
       "City                                        195254\n",
       "AQI Value                                   195254\n",
       "AQI Category                                195254\n",
       "CO AQI Value                                195254\n",
       "CO AQI Category                             195254\n",
       "Ozone AQI Value                             195254\n",
       "Ozone AQI Category                          195254\n",
       "NO2 AQI Value                               195254\n",
       "NO2 AQI Category                            195254\n",
       "PM2.5 AQI Value                             195254\n",
       "PM2.5 AQI Category                          195254\n",
       "Region                                      195254\n",
       "Reporting year                              195254\n",
       "Total emissions (metric tonnes CO2e)        195254\n",
       "Population                                  195254\n",
       "GDP                                         195254\n",
       "GDP Year                                    195254\n",
       "Average annual temperature (in Celsius)​    195254\n",
       "​Average altitude (m)                       195254\n",
       "​Land area (in square km)                   195254\n",
       "City Latitude                               195254\n",
       "City Longitude                              195254\n",
       "Country Latitude                            195254\n",
       "Country Longitude                           195254\n",
       "C40_True                                    195254\n",
       "C40_False                                   195254\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\") # save df to a pickle file so it can be used for streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0ed2a",
   "metadata": {},
   "source": [
    "#EVERYTHING ABOVE THIS CELL NEEDS TO BE RUN BEFORE TESTING ANY OF THE ALGORITHMS FOUND BELOW!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Load your DataFrame here\n",
    "\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "# Separate the features and the target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "# Hyperparameter tuning setup for the classifier after feature selection\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 150],  # Reduced number of options for speed\n",
    "    'classifier__max_depth': [None, 10],  # Simplified to speed up\n",
    "    # Simplify other parameters as needed\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with the pipeline\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to find the best model more efficiently\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Feature Importances\n",
    "if 'classifier' in best_model.named_steps:\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    features = numeric_features.tolist() + categorical_features.tolist()  # Adjust as necessary\n",
    "    feature_importance_dict = dict(zip(features, importances))\n",
    "    print(\"Feature importances:\", feature_importance_dict)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model_with_cv_and_regularization.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2086b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "# df = ...\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out()\n",
    "feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Extracting feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Display feature importances\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_path\n",
    "test_df2 = dataset_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure df is your DataFrame loaded with the data\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Mean F1-score from CV: {cv_scores.mean()} ± {cv_scores.std()}\")\n",
    "\n",
    "# Splitting data for final training and testing (optional, as cross-validation already evaluates the model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model on the entire dataset or the training set only\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names correctly after fitting the model\n",
    "try:\n",
    "    categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "except AttributeError:  # For sklearn versions prior to 0.24\n",
    "    categorical_feature_names = categorical_transformer.get_feature_names(categorical_features)\n",
    "feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Extracting and displaying feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score on the test set: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "\n",
    "# Drop 'C40_True' from the features since it's the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a full pipeline with both preprocessing and the classifier\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    # You can add more parameters here\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db41f86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country               23041\n",
       "City                  23462\n",
       "AQI Value             23463\n",
       "AQI Category          23463\n",
       "CO AQI Value          23463\n",
       "CO AQI Category       23463\n",
       "Ozone AQI Value       23463\n",
       "Ozone AQI Category    23463\n",
       "NO2 AQI Value         23463\n",
       "NO2 AQI Category      23463\n",
       "PM2.5 AQI Value       23463\n",
       "PM2.5 AQI Category    23463\n",
       "Continent             23040\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0304823b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Continent'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming df is loaded correctly\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example: df = pd.read_csv('your_data.csv')\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC40_True\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC40_False\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContinent\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC40_True\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m numeric_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Continent'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded correctly\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=15, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f805477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model is used for cross validation\n",
    "#we divide the data into 5 parts and train the model on 4 parts and test on the 5th part\n",
    "#we do this 5 times and take the average of the 5 scores\n",
    "#this is used to prevent overfitting\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded with your data\n",
    "# This is just an example setup, replace df with your actual dataframe\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=15, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Saving the model for later use\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Placeholder for loading your dataset\n",
    "# Ensure to load your actual dataset here\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1, errors='ignore')\n",
    "y = df['C40_True']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit the pipeline to train a RandomForest model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = numeric_features.tolist() + list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out())\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Save the model and feature importances\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n",
    "\n",
    "\n",
    "print(feature_importances.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the data from float to int in order to use pandas to calculate the correlations\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "df_cleaned = numeric_df.dropna()\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_cleaned.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relationships between all numerical features\n",
    "sns.pairplot(df.select_dtypes(include=['float64', 'int64']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('AQI Value')\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.scatter(df['AQI Value'], df['PM2.5 AQI Value'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8399c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AQI Value'],  label='AQI Value', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PM2.5 AQI Value'],  label='PM2.5 AQI Value', norm_hist=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'AQI Value' for each country\n",
    "country_aqi_means = df.groupby('Country')['AQI Value'].mean()\n",
    "\n",
    "# Sorting the countries by AQI value for better visualization\n",
    "country_aqi_means = country_aqi_means.sort_values()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25)) \n",
    "plt.barh(country_aqi_means.index, country_aqi_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'PM2.5 AQI Value' for each country\n",
    "country_pm25_means = df.groupby('Country')['PM2.5 AQI Value'].mean()\n",
    "# Sorting the countries by PM2.5 AQI value for better visualization\n",
    "country_pm25_means = country_pm25_means.sort_values()\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25))\n",
    "plt.barh(country_pm25_means.index, country_pm25_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average PM2.5 AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average PM2.5 AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['AQI Value'].values.reshape(-1, 1)\n",
    "y = df['PM2.5 AQI Value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.xlabel('AQI Value')\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14117185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6895792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the subsets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of Linear Regression model\n",
    "myreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to our data\n",
    "myreg.fit(X_train, y_train)\n",
    "myreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calculated coefficients\n",
    "a = myreg.coef_\n",
    "b = myreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = myreg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e01d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Linear Regression \n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X_train, a*X_train + b, color='blue')\n",
    "plt.plot(X_test, y_predicted, color='orange')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = myreg.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be591e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'Country' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='Country', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into 5 clusters using the KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df['cluster'] = kmeans.predict(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'cluster' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='cluster', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(df_filtered[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df_filtered['cluster'] = kmeans.labels_\n",
    "\n",
    "# Analyze centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Centroids:\\n\", centroids)\n",
    "\n",
    "# Plotting clusters and centroids\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_filtered, x='AQI Value', y='PM2.5 AQI Value', hue='cluster', palette='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = df[['Country', 'Population']]\n",
    "\n",
    "# Grouping and aggregating population data by country\n",
    "population_by_country = population_data.groupby('Country')['Population'].sum().reset_index()\n",
    "\n",
    "# Creating a pivot table with 'Country' as index\n",
    "pivot_population = population_by_country.set_index('Country')\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data=pivot_population, cmap='YlGnBu', annot=True, fmt=',.0f', linewidths=.5)\n",
    "plt.title('Population by Country')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Country Location' column into separate longitude and latitude columns\n",
    "df[['Latitude', 'Longitude']] = df['Country Location'].str.strip('()').str.split(', ', expand=True).astype(float)\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "scatter_plot = go.Scatter3d(\n",
    "    x=df['Longitude'],\n",
    "    y=df['Latitude'],\n",
    "    z=df['Population year'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',                # Set color to an array/list of desired values\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting layout\n",
    "layout = go.Layout(\n",
    "    title='3D Population Map',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Country Longitude'),\n",
    "        yaxis=dict(title='Country Latitude'),\n",
    "        zaxis=dict(title='Population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combining data and layout into a figure\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c549f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Country'] == 'Russian Federation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe = df[df['Country'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_europe.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_europe.sample(n=500, replace=False, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a05116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "# Load the GeoJSON data from a local file\n",
    "with open(r\"C:\\Users\\chz\\Documents\\BI Exercise\\Datasæt\\Eumap.json\", 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and already correctly set up\n",
    "m = folium.Map(location=[df_sampled['Country Latitude'].mean(), df_sampled['Country Longitude'].mean()], zoom_start=3)\n",
    "\n",
    "# Add markers for each data point\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker([row['Country Latitude'], row['Country Longitude']], popup=row['Population']).add_to(m)\n",
    "\n",
    "# Add polygon overlays for countries using the loaded GeoJSON data\n",
    "folium.GeoJson(data=geojson_data).add_to(m)\n",
    "\n",
    "# Save and display the map\n",
    "m.save('map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c45914",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f2663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
