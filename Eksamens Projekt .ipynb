{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install Streamlit\n",
    "! pip install folium\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "! pip install pycountry-convert\n",
    "! pip install streamlit-folium\n",
    "! pip install branca\n",
    "! pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8657e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for structuring the data\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for numerical analysis\n",
    "import numpy as np\n",
    "\n",
    "# import libs for diagrams inline with the text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# other utilities\n",
    "from sklearn import datasets, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d1b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# for diagramming \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For serialization and deserialization of data from/to file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b7bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9fa15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa693206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file from your data folder into a data frame\n",
    "#df = pd.read_csv(r'C:\\Users\\chz\\Documents\\BI Exercise\\\\BI Exam\\global air pollution dataset.csv')\n",
    "\n",
    "# Correctly constructing the file path\n",
    "dataset_path = os.path.join('DataSæt', 'global air pollution dataset.csv')\n",
    "dataset_path2 = os.path.join('DataSæt', '2017_-_Cities_Community_Wide_Emissions.csv')\n",
    "\n",
    "# Loading the datasets\n",
    "df = pd.read_csv(dataset_path)\n",
    "df2 = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5650ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23463, 12)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae3337fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 31)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c63edc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the correction mapping with the correct capitalization\n",
    "correction_mapping = {\n",
    "    \"United States of America\": \"USA\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Venezuela\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Republic of Korea\": \"South Korea\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    # Add other corrections as needed\n",
    "}\n",
    "\n",
    "# Apply the correction mapping to df and df2 and overwrite the original 'Country' column\n",
    "df['Country'] = df['Country'].replace(correction_mapping).str.strip()\n",
    "df2['Country'] = df2['Country'].replace(correction_mapping).str.strip()\n",
    "\n",
    "# Proceed with the merge using the corrected country names\n",
    "df_merged = pd.merge(df, df2, on='Country', how='inner')\n",
    "\n",
    "# Rename 'City_x' to 'City' and 'Country_x' to 'Country'\n",
    "df_merged.rename(columns={'City_x': 'City',}, inplace=True)\n",
    "\n",
    "# Drop the extra 'Country' column\n",
    "df_merged.drop(columns=['City_y'], inplace=True)\n",
    "\n",
    "# Rearrange the columns\n",
    "column_order = ['Country', 'City', 'AQI Value', 'AQI Category', 'CO AQI Value', 'CO AQI Category', 'Ozone AQI Value', 'Ozone AQI Category', 'NO2 AQI Value', 'NO2 AQI Category', 'PM2.5 AQI Value', 'PM2.5 AQI Category', 'Account number', 'Organization', 'Region', 'C40', 'Access', 'Reporting year', 'Accounting year', 'Boundary', 'Protocol', 'Protocol column', 'Gases included', 'Total emissions (metric tonnes CO2e)', 'Total Scope 1 Emissions (metric tonnes CO2e)', 'Total Scope 2 Emissions (metric tonnes CO2e)', 'Comment', 'Increase/Decrease from last year', 'Reason for increase/decrease in emissions', 'Population', 'Population year', 'GDP', 'GDP Currency', 'GDP Year', 'GDP Source', 'Average annual temperature (in Celsius)​', '​Average altitude (m)', '​Land area (in square km)', 'City Location', 'Country Location']\n",
    "# Reorder the DataFrame columns\n",
    "df_merged = df_merged[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f54e8b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288622, 40)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91a3c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                              0\n",
       "City                                                 1\n",
       "AQI Value                                            0\n",
       "AQI Category                                         0\n",
       "CO AQI Value                                         0\n",
       "CO AQI Category                                      0\n",
       "Ozone AQI Value                                      0\n",
       "Ozone AQI Category                                   0\n",
       "NO2 AQI Value                                        0\n",
       "NO2 AQI Category                                     0\n",
       "PM2.5 AQI Value                                      0\n",
       "PM2.5 AQI Category                                   0\n",
       "Account number                                       0\n",
       "Organization                                         0\n",
       "Region                                               0\n",
       "C40                                             237710\n",
       "Access                                               0\n",
       "Reporting year                                       0\n",
       "Accounting year                                      0\n",
       "Boundary                                             0\n",
       "Protocol                                             0\n",
       "Protocol column                                 114244\n",
       "Gases included                                  149081\n",
       "Total emissions (metric tonnes CO2e)             20562\n",
       "Total Scope 1 Emissions (metric tonnes CO2e)     57609\n",
       "Total Scope 2 Emissions (metric tonnes CO2e)     61085\n",
       "Comment                                         191552\n",
       "Increase/Decrease from last year                 16018\n",
       "Reason for increase/decrease in emissions        63321\n",
       "Population                                           0\n",
       "Population year                                      0\n",
       "GDP                                              60666\n",
       "GDP Currency                                     58996\n",
       "GDP Year                                         59442\n",
       "GDP Source                                       58727\n",
       "Average annual temperature (in Celsius)​          5394\n",
       "​Average altitude (m)                            20526\n",
       "​Land area (in square km)                         1562\n",
       "City Location                                        0\n",
       "Country Location                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0c8ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Gases included'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol column'], inplace=True)\n",
    "df_merged.drop(columns=['Comment'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 1 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Total Scope 2 Emissions (metric tonnes CO2e)'], inplace=True)\n",
    "df_merged.drop(columns=['Account number'], inplace=True)\n",
    "df_merged.drop(columns=['Organization'], inplace=True)\n",
    "df_merged.drop(columns=['Accounting year'], inplace=True)\n",
    "df_merged.drop(columns=['Boundary'], inplace=True)\n",
    "df_merged.drop(columns=['Protocol'], inplace=True)\n",
    "df_merged.drop(columns=['Increase/Decrease from last year'], inplace=True)\n",
    "df_merged.drop(columns=['Reason for increase/decrease in emissions'], inplace=True)\n",
    "df_merged.drop(columns=['Population year'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Currency'], inplace=True)\n",
    "df_merged.drop(columns=['GDP Source'], inplace=True)\n",
    "df_merged.drop(columns=['Access'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57d1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nulls/NaNs to 'False'\n",
    "df_merged['C40'] = df_merged['C40'].fillna('False')\n",
    "\n",
    "# Convert any cell that contains \"C40\" to 'True', assuming \"C40\" indicates a true condition\n",
    "# Adjust the condition as needed to match your data's specific representation of true\n",
    "df_merged['C40'] = df_merged['C40'].apply(lambda x: 'True' if 'C40' in str(x) else 'False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f4054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values from the data frame\n",
    "df_merged = df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32c3ad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     0\n",
       "City                                        0\n",
       "AQI Value                                   0\n",
       "AQI Category                                0\n",
       "CO AQI Value                                0\n",
       "CO AQI Category                             0\n",
       "Ozone AQI Value                             0\n",
       "Ozone AQI Category                          0\n",
       "NO2 AQI Value                               0\n",
       "NO2 AQI Category                            0\n",
       "PM2.5 AQI Value                             0\n",
       "PM2.5 AQI Category                          0\n",
       "Region                                      0\n",
       "C40                                         0\n",
       "Reporting year                              0\n",
       "Total emissions (metric tonnes CO2e)        0\n",
       "Population                                  0\n",
       "GDP                                         0\n",
       "GDP Year                                    0\n",
       "Average annual temperature (in Celsius)​    0\n",
       "​Average altitude (m)                       0\n",
       "​Land area (in square km)                   0\n",
       "City Location                               0\n",
       "Country Location                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data frame for null values\n",
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f2328ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195190\n",
       "City                                        195190\n",
       "AQI Value                                   195190\n",
       "AQI Category                                195190\n",
       "CO AQI Value                                195190\n",
       "CO AQI Category                             195190\n",
       "Ozone AQI Value                             195190\n",
       "Ozone AQI Category                          195190\n",
       "NO2 AQI Value                               195190\n",
       "NO2 AQI Category                            195190\n",
       "PM2.5 AQI Value                             195190\n",
       "PM2.5 AQI Category                          195190\n",
       "Region                                      195190\n",
       "C40                                         195190\n",
       "Reporting year                              195190\n",
       "Total emissions (metric tonnes CO2e)        195190\n",
       "Population                                  195190\n",
       "GDP                                         195190\n",
       "GDP Year                                    195190\n",
       "Average annual temperature (in Celsius)​    195190\n",
       "​Average altitude (m)                       195190\n",
       "​Land area (in square km)                   195190\n",
       "City Location                               195190\n",
       "Country Location                            195190\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbf8382b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Location</th>\n",
       "      <th>Country Location</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>(-12.97304, -38.502304)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-12.97304</td>\n",
       "      <td>-38.502304</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.1</td>\n",
       "      <td>(-22.892857, -43.118381)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-22.892857</td>\n",
       "      <td>-43.118381</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>(-27.5949884, -48.5481743)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-27.5949884</td>\n",
       "      <td>-48.5481743</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>(-19.916681, -43.934493)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-19.916681</td>\n",
       "      <td>-43.934493</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Presidente Dutra</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>749.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>(-16.6868912, -49.2647943)</td>\n",
       "      <td>(-14.235004, -51.92528)</td>\n",
       "      <td>-16.6868912</td>\n",
       "      <td>-49.2647943</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.92528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country              City  AQI Value AQI Category  CO AQI Value  \\\n",
       "1241  Brazil  Presidente Dutra         41         Good             1   \n",
       "1242  Brazil  Presidente Dutra         41         Good             1   \n",
       "1243  Brazil  Presidente Dutra         41         Good             1   \n",
       "1244  Brazil  Presidente Dutra         41         Good             1   \n",
       "1246  Brazil  Presidente Dutra         41         Good             1   \n",
       "\n",
       "     CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "1241            Good                5               Good              1   \n",
       "1242            Good                5               Good              1   \n",
       "1243            Good                5               Good              1   \n",
       "1244            Good                5               Good              1   \n",
       "1246            Good                5               Good              1   \n",
       "\n",
       "     NO2 AQI Category  ...  GDP Year Average annual temperature (in Celsius)​  \\\n",
       "1241             Good  ...    2012.0                                     26.0   \n",
       "1242             Good  ...    2013.0                                     23.0   \n",
       "1243             Good  ...    2013.0                                     20.0   \n",
       "1244             Good  ...    2014.0                                     21.0   \n",
       "1246             Good  ...    2010.0                                     23.2   \n",
       "\n",
       "     ​Average altitude (m) ​Land area (in square km)  \\\n",
       "1241                   8.0                     692.0   \n",
       "1242                   5.0                     133.1   \n",
       "1243                   3.0                     438.0   \n",
       "1244                 900.0                     331.0   \n",
       "1246                 749.0                     739.0   \n",
       "\n",
       "                   City Location         Country Location  City Latitude  \\\n",
       "1241     (-12.97304, -38.502304)  (-14.235004, -51.92528)      -12.97304   \n",
       "1242    (-22.892857, -43.118381)  (-14.235004, -51.92528)     -22.892857   \n",
       "1243  (-27.5949884, -48.5481743)  (-14.235004, -51.92528)    -27.5949884   \n",
       "1244    (-19.916681, -43.934493)  (-14.235004, -51.92528)     -19.916681   \n",
       "1246  (-16.6868912, -49.2647943)  (-14.235004, -51.92528)    -16.6868912   \n",
       "\n",
       "      City Longitude  Country Latitude  Country Longitude  \n",
       "1241      -38.502304        -14.235004          -51.92528  \n",
       "1242      -43.118381        -14.235004          -51.92528  \n",
       "1243     -48.5481743        -14.235004          -51.92528  \n",
       "1244      -43.934493        -14.235004          -51.92528  \n",
       "1246     -49.2647943        -14.235004          -51.92528  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting latitude and longitude from \"City Location\" and \"Country Location\" into new columns\n",
    "df_merged[['City Latitude', 'City Longitude']] = df_merged['City Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "df_merged[['Country Latitude', 'Country Longitude']] = df_merged['Country Location'].str.extract(r'\\(([^,]+), ([^)]+)\\)')\n",
    "\n",
    "# Displaying the first few rows to ensure the transformation was successful\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be5b9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the latitude and longitude columns from strings to floats\n",
    "df_merged['City Latitude'] = pd.to_numeric(df_merged['City Latitude'], errors='coerce')\n",
    "df_merged['City Longitude'] = pd.to_numeric(df_merged['City Longitude'], errors='coerce')\n",
    "df_merged['Country Latitude'] = pd.to_numeric(df_merged['Country Latitude'], errors='coerce')\n",
    "df_merged['Country Longitude'] = pd.to_numeric(df_merged['Country Longitude'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b08ec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['City Location'], inplace=True)\n",
    "df_merged.drop(columns=['Country Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c658a0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'C40' from strings \"True\"/\"False\" to actual booleans\n",
    "df_merged['C40'] = df_merged['C40'].map({'True': True, 'False': False})\n",
    "\n",
    "# Create two new columns: 'C40_True' and 'C40_False'\n",
    "df_merged['C40_True'] = df_merged['C40'].astype(int)  # This will convert True to 1 and False to 0\n",
    "df_merged['C40_False'] = (~df_merged['C40']).astype(int)  # This inverts the boolean and then converts to 0/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a58e5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['C40'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b97ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf32c49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP Year</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125954</th>\n",
       "      <td>USA</td>\n",
       "      <td>Norridge</td>\n",
       "      <td>62</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>27</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>30.2672</td>\n",
       "      <td>-97.7431</td>\n",
       "      <td>37.09024</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country      City  AQI Value AQI Category  CO AQI Value  \\\n",
       "125954     USA  Norridge         62     Moderate             1   \n",
       "\n",
       "       CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "125954            Good               27               Good              6   \n",
       "\n",
       "       NO2 AQI Category  ...  GDP Year  \\\n",
       "125954             Good  ...    2015.0   \n",
       "\n",
       "       Average annual temperature (in Celsius)​ ​Average altitude (m)  \\\n",
       "125954                                     20.0                 149.0   \n",
       "\n",
       "        ​Land area (in square km)  City Latitude  City Longitude  \\\n",
       "125954                      704.0        30.2672        -97.7431   \n",
       "\n",
       "        Country Latitude  Country Longitude  C40_True  C40_False  \n",
       "125954          37.09024         -95.712891         1          0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba503efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195190 entries, 1241 to 288621\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Country                                   195190 non-null  object \n",
      " 1   City                                      195190 non-null  object \n",
      " 2   AQI Value                                 195190 non-null  int64  \n",
      " 3   AQI Category                              195190 non-null  object \n",
      " 4   CO AQI Value                              195190 non-null  int64  \n",
      " 5   CO AQI Category                           195190 non-null  object \n",
      " 6   Ozone AQI Value                           195190 non-null  int64  \n",
      " 7   Ozone AQI Category                        195190 non-null  object \n",
      " 8   NO2 AQI Value                             195190 non-null  int64  \n",
      " 9   NO2 AQI Category                          195190 non-null  object \n",
      " 10  PM2.5 AQI Value                           195190 non-null  int64  \n",
      " 11  PM2.5 AQI Category                        195190 non-null  object \n",
      " 12  Region                                    195190 non-null  object \n",
      " 13  Reporting year                            195190 non-null  int64  \n",
      " 14  Total emissions (metric tonnes CO2e)      195190 non-null  float64\n",
      " 15  Population                                195190 non-null  int64  \n",
      " 16  GDP                                       195190 non-null  float64\n",
      " 17  GDP Year                                  195190 non-null  float64\n",
      " 18  Average annual temperature (in Celsius)​  195190 non-null  float64\n",
      " 19  ​Average altitude (m)                     195190 non-null  float64\n",
      " 20  ​Land area (in square km)                 195190 non-null  float64\n",
      " 21  City Latitude                             195190 non-null  float64\n",
      " 22  City Longitude                            195190 non-null  float64\n",
      " 23  Country Latitude                          195190 non-null  float64\n",
      " 24  Country Longitude                         195190 non-null  float64\n",
      " 25  C40_True                                  195190 non-null  int32  \n",
      " 26  C40_False                                 195190 non-null  int32  \n",
      "dtypes: float64(10), int32(2), int64(7), object(8)\n",
      "memory usage: 40.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "761cee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "#applying continent to the dataset for future use of folium mapping\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        return country_continent_name\n",
    "    except:\n",
    "        return None  # For countries that don't match\n",
    "\n",
    "# Apply the conversion function to your DataFrame\n",
    "df['Continent'] = df['Country'].apply(country_to_continent)\n",
    "# Filter for other continents\n",
    "north_american_countries_df = df[df['Continent'] == 'North America']\n",
    "south_american_countries_df = df[df['Continent'] == 'South America']\n",
    "asian_countries_df = df[df['Continent'] == 'Asia']\n",
    "african_countries_df = df[df['Continent'] == 'Africa']\n",
    "oceania_countries_df = df[df['Continent'] == 'Oceania']\n",
    "Europe_df = df[df['Continent'] == 'Europe']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a16c832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     195190\n",
       "City                                        195190\n",
       "AQI Value                                   195190\n",
       "AQI Category                                195190\n",
       "CO AQI Value                                195190\n",
       "CO AQI Category                             195190\n",
       "Ozone AQI Value                             195190\n",
       "Ozone AQI Category                          195190\n",
       "NO2 AQI Value                               195190\n",
       "NO2 AQI Category                            195190\n",
       "PM2.5 AQI Value                             195190\n",
       "PM2.5 AQI Category                          195190\n",
       "Region                                      195190\n",
       "Reporting year                              195190\n",
       "Total emissions (metric tonnes CO2e)        195190\n",
       "Population                                  195190\n",
       "GDP                                         195190\n",
       "GDP Year                                    195190\n",
       "Average annual temperature (in Celsius)​    195190\n",
       "​Average altitude (m)                       195190\n",
       "​Land area (in square km)                   195190\n",
       "City Latitude                               195190\n",
       "City Longitude                              195190\n",
       "Country Latitude                            195190\n",
       "Country Longitude                           195190\n",
       "C40_True                                    195190\n",
       "C40_False                                   195190\n",
       "Continent                                   195189\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e50160f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AQI Value</th>\n",
       "      <th>AQI Category</th>\n",
       "      <th>CO AQI Value</th>\n",
       "      <th>CO AQI Category</th>\n",
       "      <th>Ozone AQI Value</th>\n",
       "      <th>Ozone AQI Category</th>\n",
       "      <th>NO2 AQI Value</th>\n",
       "      <th>NO2 AQI Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Average annual temperature (in Celsius)​</th>\n",
       "      <th>​Average altitude (m)</th>\n",
       "      <th>​Land area (in square km)</th>\n",
       "      <th>City Latitude</th>\n",
       "      <th>City Longitude</th>\n",
       "      <th>Country Latitude</th>\n",
       "      <th>Country Longitude</th>\n",
       "      <th>C40_True</th>\n",
       "      <th>C40_False</th>\n",
       "      <th>Continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22487</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Angatuba</td>\n",
       "      <td>18</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>5780.0</td>\n",
       "      <td>-15.794229</td>\n",
       "      <td>-47.882166</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.925280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Palmas</td>\n",
       "      <td>163</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>-25.431063</td>\n",
       "      <td>-49.264693</td>\n",
       "      <td>-14.235004</td>\n",
       "      <td>-51.925280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192378</th>\n",
       "      <td>USA</td>\n",
       "      <td>Speedway</td>\n",
       "      <td>56</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>39</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>34.000700</td>\n",
       "      <td>-81.034800</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70523</th>\n",
       "      <td>USA</td>\n",
       "      <td>Brooklyn Center</td>\n",
       "      <td>51</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>35.199200</td>\n",
       "      <td>-111.631100</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97924</th>\n",
       "      <td>USA</td>\n",
       "      <td>Colchester</td>\n",
       "      <td>39</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>24</td>\n",
       "      <td>Good</td>\n",
       "      <td>9</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.195000</td>\n",
       "      <td>-106.837000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49690</th>\n",
       "      <td>USA</td>\n",
       "      <td>Annandale</td>\n",
       "      <td>50</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>50</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>39.103100</td>\n",
       "      <td>-84.512000</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185831</th>\n",
       "      <td>USA</td>\n",
       "      <td>Topsham</td>\n",
       "      <td>46</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>21</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4</td>\n",
       "      <td>199.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>41.499300</td>\n",
       "      <td>-81.694400</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273960</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Carluke</td>\n",
       "      <td>31</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>15</td>\n",
       "      <td>Good</td>\n",
       "      <td>8</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>51.481581</td>\n",
       "      <td>-3.179090</td>\n",
       "      <td>55.378051</td>\n",
       "      <td>-3.435973</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88928</th>\n",
       "      <td>USA</td>\n",
       "      <td>Gautier</td>\n",
       "      <td>51</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>42</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.724930</td>\n",
       "      <td>-122.156077</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218024</th>\n",
       "      <td>USA</td>\n",
       "      <td>Fernley</td>\n",
       "      <td>34</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>34</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>35.199200</td>\n",
       "      <td>-111.631100</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Country             City  AQI Value AQI Category  CO AQI Value  \\\n",
       "22487           Brazil         Angatuba         18         Good             1   \n",
       "16354           Brazil           Palmas        163    Unhealthy             6   \n",
       "192378             USA         Speedway         56     Moderate             1   \n",
       "70523              USA  Brooklyn Center         51     Moderate             1   \n",
       "97924              USA       Colchester         39         Good             1   \n",
       "49690              USA        Annandale         50         Good             1   \n",
       "185831             USA          Topsham         46         Good             1   \n",
       "273960  United Kingdom          Carluke         31         Good             1   \n",
       "88928              USA          Gautier         51     Moderate             1   \n",
       "218024             USA          Fernley         34         Good             1   \n",
       "\n",
       "       CO AQI Category  Ozone AQI Value Ozone AQI Category  NO2 AQI Value  \\\n",
       "22487             Good                8               Good              1   \n",
       "16354             Good                4               Good              3   \n",
       "192378            Good               39               Good              2   \n",
       "70523             Good               31               Good              6   \n",
       "97924             Good               24               Good              9   \n",
       "49690             Good               50               Good              1   \n",
       "185831            Good               21               Good              6   \n",
       "273960            Good               15               Good              8   \n",
       "88928             Good               42               Good              1   \n",
       "218024            Good               34               Good              3   \n",
       "\n",
       "       NO2 AQI Category  ...  Average annual temperature (in Celsius)​  \\\n",
       "22487              Good  ...                                      21.0   \n",
       "16354              Good  ...                                      18.0   \n",
       "192378             Good  ...                                      32.0   \n",
       "70523              Good  ...                                       6.5   \n",
       "97924              Good  ...                                       6.0   \n",
       "49690              Good  ...                                      13.0   \n",
       "185831             Good  ...                                      10.4   \n",
       "273960             Good  ...                                       9.7   \n",
       "88928              Good  ...                                      15.0   \n",
       "218024             Good  ...                                       6.5   \n",
       "\n",
       "       ​Average altitude (m) ​Land area (in square km)  City Latitude  \\\n",
       "22487                 1172.0                    5780.0     -15.794229   \n",
       "16354                  935.0                     434.0     -25.431063   \n",
       "192378                 292.0                     132.0      34.000700   \n",
       "70523                 2134.0                     165.0      35.199200   \n",
       "97924                 2405.0                      20.0      39.195000   \n",
       "49690                  168.0                     124.0      39.103100   \n",
       "185831                 199.0                     201.0      41.499300   \n",
       "273960                  65.0                     140.0      51.481581   \n",
       "88928                   15.0                      40.0      37.724930   \n",
       "218024                2134.0                     165.0      35.199200   \n",
       "\n",
       "        City Longitude  Country Latitude  Country Longitude  C40_True  \\\n",
       "22487       -47.882166        -14.235004         -51.925280         0   \n",
       "16354       -49.264693        -14.235004         -51.925280         1   \n",
       "192378      -81.034800         37.090240         -95.712891         0   \n",
       "70523      -111.631100         37.090240         -95.712891         0   \n",
       "97924      -106.837000         37.090240         -95.712891         0   \n",
       "49690       -84.512000         37.090240         -95.712891         0   \n",
       "185831      -81.694400         37.090240         -95.712891         0   \n",
       "273960       -3.179090         55.378051          -3.435973         0   \n",
       "88928      -122.156077         37.090240         -95.712891         0   \n",
       "218024     -111.631100         37.090240         -95.712891         0   \n",
       "\n",
       "        C40_False      Continent  \n",
       "22487           1  South America  \n",
       "16354           0  South America  \n",
       "192378          1  North America  \n",
       "70523           1  North America  \n",
       "97924           1  North America  \n",
       "49690           1  North America  \n",
       "185831          1  North America  \n",
       "273960          1         Europe  \n",
       "88928           1  North America  \n",
       "218024          1  North America  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7eab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ce03091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                     13408\n",
       "City                                        13408\n",
       "AQI Value                                   13408\n",
       "AQI Category                                13408\n",
       "CO AQI Value                                13408\n",
       "CO AQI Category                             13408\n",
       "Ozone AQI Value                             13408\n",
       "Ozone AQI Category                          13408\n",
       "NO2 AQI Value                               13408\n",
       "NO2 AQI Category                            13408\n",
       "PM2.5 AQI Value                             13408\n",
       "PM2.5 AQI Category                          13408\n",
       "Region                                      13408\n",
       "Reporting year                              13408\n",
       "Total emissions (metric tonnes CO2e)        13408\n",
       "Population                                  13408\n",
       "GDP                                         13408\n",
       "GDP Year                                    13408\n",
       "Average annual temperature (in Celsius)​    13408\n",
       "​Average altitude (m)                       13408\n",
       "​Land area (in square km)                   13408\n",
       "City Latitude                               13408\n",
       "City Longitude                              13408\n",
       "Country Latitude                            13408\n",
       "Country Longitude                           13408\n",
       "C40_True                                    13408\n",
       "C40_False                                   13408\n",
       "Continent                                   13407\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c48d5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dataframe.pkl\") # save df to a pickle file so it can be used for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65628641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__n_estimators': 100, 'classifier__max_depth': None}\n",
      "Best cross-validation score: 0.96\n",
      "Test set accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1058\n",
      "           1       1.00      1.00      1.00      1624\n",
      "\n",
      "    accuracy                           1.00      2682\n",
      "   macro avg       1.00      1.00      1.00      2682\n",
      "weighted avg       1.00      1.00      1.00      2682\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model.joblib']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Load your DataFrame here\n",
    "\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "# Separate the features and the target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "# Hyperparameter tuning setup for the classifier after feature selection\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 150],  # Reduced number of options for speed\n",
    "    'classifier__max_depth': [None, 10],  # Simplified to speed up\n",
    "    # Simplify other parameters as needed\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with the pipeline\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to find the best model more efficiently\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39fb56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters: {'classifier__n_estimators': 300, 'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 20}\n",
      "Best score: 0.9698619917941066\n",
      "Feature importances: {'AQI Value': 0.002151142947225884, 'CO AQI Value': 0.0011112290880497353, 'Ozone AQI Value': 0.0031515654321826107, 'NO2 AQI Value': 0.0011785809060354377, 'PM2.5 AQI Value': 0.002498602891590687, 'Reporting year': 0.0, 'Total emissions (metric tonnes CO2e)': 0.18690858773366964, 'Population': 0.2111117331019797, 'GDP': 0.06489977782250016, 'GDP Year': 0.034019572445656045, 'Average annual temperature (in Celsius)\\u200b': 0.05815023660238964, '\\u200bAverage altitude (m)': 0.06661833040826666, '\\u200bLand area (in square km)': 0.047345581728997256, 'City Latitude': 0.05130212255961181, 'City Longitude': 0.08949903223025225, 'Country Latitude': 0.04320868771811754, 'Country Longitude': 0.08543809293857652, 'AQI Category': 0.0006686978782536167, 'CO AQI Category': 0.0, 'Ozone AQI Category': 0.00016815070782901634, 'NO2 AQI Category': 0.00015402445018646173, 'PM2.5 AQI Category': 1.954934601108261e-05, 'Region': 4.478864453745468e-05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model_with_cv_and_regularization.joblib']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', rf)])\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Feature Importances\n",
    "if 'classifier' in best_model.named_steps:\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    features = numeric_features.tolist() + categorical_features.tolist()  # Adjust as necessary\n",
    "    feature_importance_dict = dict(zip(features, importances))\n",
    "    print(\"Feature importances:\", feature_importance_dict)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_rf_model_with_cv_and_regularization.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2086b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Extracting feature names\u001b[39;00m\n\u001b[0;32m     53\u001b[0m numeric_feature_names \u001b[38;5;241m=\u001b[39m numeric_features\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:726\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    723\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    724\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 726\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    727\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    729\u001b[0m )\n\u001b[0;32m    731\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    732\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:727\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    723\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    724\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    726\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 727\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    729\u001b[0m )\n\u001b[0;32m    731\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    732\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:32\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     30\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     31\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m     33\u001b[0m     X_train,\n\u001b[0;32m     34\u001b[0m     y_train,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m estimator, features: _score(\n\u001b[0;32m     36\u001b[0m         estimator, X_test[:, features], y_test, scorer\n\u001b[0;32m     37\u001b[0m     ),\n\u001b[0;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:297\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 297\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    300\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    301\u001b[0m     estimator,\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    303\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    304\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1302\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1300\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1302\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[0;32m   1303\u001b[0m     path_func(\n\u001b[0;32m   1304\u001b[0m         X,\n\u001b[0;32m   1305\u001b[0m         y,\n\u001b[0;32m   1306\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[0;32m   1307\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[0;32m   1308\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[0;32m   1309\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1310\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1311\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1312\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[0;32m   1313\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[0;32m   1314\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1315\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1316\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1317\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1318\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[0;32m   1319\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[0;32m   1320\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[0;32m   1321\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1322\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m   1323\u001b[0m     )\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[0;32m   1325\u001b[0m )\n\u001b[0;32m   1327\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:452\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    448\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[0;32m    449\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    450\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    451\u001b[0m ]\n\u001b[1;32m--> 452\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    453\u001b[0m     func,\n\u001b[0;32m    454\u001b[0m     w0,\n\u001b[0;32m    455\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    456\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    457\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    458\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: iprint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: tol, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_iter},\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    460\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    461\u001b[0m     solver,\n\u001b[0;32m    462\u001b[0m     opt_res,\n\u001b[0;32m    463\u001b[0m     max_iter,\n\u001b[0;32m    464\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    465\u001b[0m )\n\u001b[0;32m    466\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:279\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[1;32m--> 279\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_loss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    280\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    281\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    282\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    283\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    285\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    286\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n",
      "File \u001b[1;32mc:\\Users\\chz\\anaconda3\\Lib\\site-packages\\sklearn\\_loss\\loss.py:253\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[1;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m gradient_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    251\u001b[0m     gradient_out \u001b[38;5;241m=\u001b[39m gradient_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    254\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my_true,\n\u001b[0;32m    255\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    256\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    257\u001b[0m     loss_out\u001b[38;5;241m=\u001b[39mloss_out,\n\u001b[0;32m    258\u001b[0m     gradient_out\u001b[38;5;241m=\u001b[39mgradient_out,\n\u001b[0;32m    259\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    260\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "# df = ...\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out()\n",
    "feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Extracting feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Display feature importances\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_path\n",
    "test_df2 = dataset_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2998b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-score from CV: 0.958847857762712 ± 0.08230428447457605\n",
      "                 Feature  Importance\n",
      "0        PM2.5 AQI Value    0.228065\n",
      "1          NO2 AQI Value    0.208148\n",
      "2  ​Average altitude (m)    0.114804\n",
      "3          City Latitude    0.095064\n",
      "4         Reporting year    0.057321\n",
      "Model F1-score on the test set: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1058\n",
      "           1       1.00      1.00      1.00      1624\n",
      "\n",
      "    accuracy                           1.00      2682\n",
      "   macro avg       1.00      1.00      1.00      2682\n",
      "weighted avg       1.00      1.00      1.00      2682\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature_importances.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensure df is your DataFrame loaded with the data\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection integrated within the classifier pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=5, scoring='f1')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Mean F1-score from CV: {cv_scores.mean()} ± {cv_scores.std()}\")\n",
    "\n",
    "# Splitting data for final training and testing (optional, as cross-validation already evaluates the model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Training the model on the entire dataset or the training set only\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature names correctly after fitting the model\n",
    "try:\n",
    "    categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "except AttributeError:  # For sklearn versions prior to 0.24\n",
    "    categorical_feature_names = categorical_transformer.get_feature_names(categorical_features)\n",
    "feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Extracting and displaying feature importances\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_importances = pd.DataFrame(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True), columns=['Feature', 'Importance'])\n",
    "print(feature_importances.head())\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Model F1-score on the test set: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Saving the model and feature importances for later use\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d184f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__n_estimators': 200}\n",
      "Best cross-validation score: 0.97\n",
      "Test set accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1058\n",
      "           1       1.00      1.00      1.00      1624\n",
      "\n",
      "    accuracy                           1.00      2682\n",
      "   macro avg       1.00      1.00      1.00      2682\n",
      "weighted avg       1.00      1.00      1.00      2682\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_rf_model.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your DataFrame here\n",
    "# For example: df = pd.read_csv('your_data.csv')\n",
    "# Make sure to replace this with your actual data loading code\n",
    "\n",
    "\n",
    "# Drop 'C40_True' from the features since it's the target variable\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "# Identifying numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a full pipeline with both preprocessing and the classifier\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    # You can add more parameters here\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model for later use\n",
    "joblib.dump(best_model, 'best_rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0304823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [1.         1.         1.         1.         0.79597165]\n",
      "Mean Accuracy: 0.9591943304737038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['less_overfitting_model.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded correctly\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f805477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [1.         1.         1.         1.         0.79597165]\n",
      "Mean Accuracy: 0.9591943304737038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['less_overfitting_model.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Assuming df is loaded with your data\n",
    "# This is just an example setup, replace df with your actual dataframe\n",
    "\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1)\n",
    "y = df['C40_True']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Adjusted RandomForestClassifier parameters to prevent overfitting\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_depth=10, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', RFECV(estimator=LogisticRegression(max_iter=1000), step=1, cv=10, scoring='accuracy')),\n",
    "    ('classifier', rf)])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "# Saving the model for later use\n",
    "joblib.dump(pipeline, 'less_overfitting_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9edfd4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Feature  Importance\n",
      "7                             Population    0.229103\n",
      "6   Total emissions (metric tonnes CO2e)    0.156194\n",
      "16                     Country Longitude    0.091324\n",
      "14                        City Longitude    0.086150\n",
      "11                 ​Average altitude (m)    0.062751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Placeholder for loading your dataset\n",
    "# Ensure to load your actual dataset here\n",
    "# df = pd.read_csv('path/to/your_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['C40_True', 'C40_False', 'Country', 'City', 'Continent'], axis=1, errors='ignore')\n",
    "y = df['C40_True']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Fit the pipeline to train a RandomForest model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = numeric_features.tolist() + list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out())\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Save the model and feature importances\n",
    "joblib.dump(pipeline, 'finalized_model.joblib')\n",
    "joblib.dump(feature_importances, 'feature_importances.joblib')\n",
    "\n",
    "\n",
    "print(feature_importances.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the data from float to int in order to use pandas to calculate the correlations\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "\n",
    "df_cleaned = numeric_df.dropna()\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_cleaned.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relationships between all numerical features\n",
    "sns.pairplot(df.select_dtypes(include=['float64', 'int64']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('AQI Value')\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.scatter(df['AQI Value'], df['PM2.5 AQI Value'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8399c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AQI Value'],  label='AQI Value', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['PM2.5 AQI Value'],  label='PM2.5 AQI Value', norm_hist=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'AQI Value' for each country\n",
    "country_aqi_means = df.groupby('Country')['AQI Value'].mean()\n",
    "\n",
    "# Sorting the countries by AQI value for better visualization\n",
    "country_aqi_means = country_aqi_means.sort_values()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25)) \n",
    "plt.barh(country_aqi_means.index, country_aqi_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by 'Country' and calculating the mean 'PM2.5 AQI Value' for each country\n",
    "country_pm25_means = df.groupby('Country')['PM2.5 AQI Value'].mean()\n",
    "# Sorting the countries by PM2.5 AQI value for better visualization\n",
    "country_pm25_means = country_pm25_means.sort_values()\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(15, 25))\n",
    "plt.barh(country_pm25_means.index, country_pm25_means.values, color='skyblue') # Horizontal bar chart\n",
    "plt.xlabel('Average PM2.5 AQI Value')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Average PM2.5 AQI Value by Country')\n",
    "plt.tight_layout() # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['AQI Value'].values.reshape(-1, 1)\n",
    "y = df['PM2.5 AQI Value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all\n",
    "plt.ylabel('PM2.5 AQI Value')\n",
    "plt.xlabel('AQI Value')\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14117185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6895792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the subsets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of Linear Regression model\n",
    "myreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to our data\n",
    "myreg.fit(X_train, y_train)\n",
    "myreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calculated coefficients\n",
    "a = myreg.coef_\n",
    "b = myreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = myreg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e01d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Linear Regression \n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X_train, a*X_train + b, color='blue')\n",
    "plt.plot(X_test, y_predicted, color='orange')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = myreg.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be591e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'Country' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='Country', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into 5 clusters using the KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df['cluster'] = kmeans.predict(df[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the 'AQI Value' and 'PM2.5 AQI Value' columns and color the points by the 'cluster' column\n",
    "fig = px.scatter(df, x='AQI Value', y='PM2.5 AQI Value', color='cluster', title='AQI Value vs PM2.5 AQI Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(df_filtered[['AQI Value', 'PM2.5 AQI Value']])\n",
    "df_filtered['cluster'] = kmeans.labels_\n",
    "\n",
    "# Analyze centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Centroids:\\n\", centroids)\n",
    "\n",
    "# Plotting clusters and centroids\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_filtered, x='AQI Value', y='PM2.5 AQI Value', hue='cluster', palette='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = df[['Country', 'Population']]\n",
    "\n",
    "# Grouping and aggregating population data by country\n",
    "population_by_country = population_data.groupby('Country')['Population'].sum().reset_index()\n",
    "\n",
    "# Creating a pivot table with 'Country' as index\n",
    "pivot_population = population_by_country.set_index('Country')\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data=pivot_population, cmap='YlGnBu', annot=True, fmt=',.0f', linewidths=.5)\n",
    "plt.title('Population by Country')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Country Location' column into separate longitude and latitude columns\n",
    "df[['Latitude', 'Longitude']] = df['Country Location'].str.strip('()').str.split(', ', expand=True).astype(float)\n",
    "\n",
    "# Creating a 3D scatter plot\n",
    "scatter_plot = go.Scatter3d(\n",
    "    x=df['Longitude'],\n",
    "    y=df['Latitude'],\n",
    "    z=df['Population year'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',                # Set color to an array/list of desired values\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Setting layout\n",
    "layout = go.Layout(\n",
    "    title='3D Population Map',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Country Longitude'),\n",
    "        yaxis=dict(title='Country Latitude'),\n",
    "        zaxis=dict(title='Population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combining data and layout into a figure\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c549f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['Country'] == 'Russian Federation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe = df[df['Country'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_europe.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_europe.sample(n=500, replace=False, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a05116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "# Load the GeoJSON data from a local file\n",
    "with open(r\"C:\\Users\\chz\\Documents\\BI Exercise\\Datasæt\\Eumap.json\", 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and already correctly set up\n",
    "m = folium.Map(location=[df_sampled['Country Latitude'].mean(), df_sampled['Country Longitude'].mean()], zoom_start=3)\n",
    "\n",
    "# Add markers for each data point\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker([row['Country Latitude'], row['Country Longitude']], popup=row['Population']).add_to(m)\n",
    "\n",
    "# Add polygon overlays for countries using the loaded GeoJSON data\n",
    "folium.GeoJson(data=geojson_data).add_to(m)\n",
    "\n",
    "# Save and display the map\n",
    "m.save('map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c45914",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f2663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
